{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkBootstrapping.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bazarum/SparkCourse2019/blob/master/SparkBootstrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kvD4HBMi0ohY"
      },
      "cell_type": "markdown",
      "source": [
        "# Spark Bootstraping for Google Colab\n",
        "\n",
        "Run this before start working with the notebooks from the spark course. \n",
        "When you will start a new (and fresh) notebook at Colab. Google Cloud will create a new Docker container just for your use. \n",
        "\n",
        "Executing this notebook will install into the container the software. The container will be reused by the user until it will destroy by inactivity.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fUhBhrGmyAvs",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "b4Kjvk_h1AHl"
      },
      "cell_type": "markdown",
      "source": [
        "# Set Environment Variables\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8Xnb_ePUyQIL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\"\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[2] pyspark-shell\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NwU28K5f1H3P"
      },
      "cell_type": "markdown",
      "source": [
        "# Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zgReRGl0y23D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1725d86c-844d-4bea-bdfd-ce146dae88e1"
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "print(findspark.init())\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "T3ULPx4Y1LiR"
      },
      "cell_type": "markdown",
      "source": [
        "# Use Spark!\n",
        "That's all there is to it - you're ready to use Spark!"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XJp8ZI-VzYEz",
        "outputId": "37e59663-1931-45fe-c309-9dfddcd3388e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PZkw_gPEQvId",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "49113265-e88a-4d21-ba0c-65d6b6a4242d"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bazarum/SparkCourse2019.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SparkCourse2019'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 37 (delta 4), reused 31 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (37/37), done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}