{
  "paragraphs": [
    {
      "text": "%md\nDatasets use with this tutorial\n---\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 9:38:51 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eDatasets use with this tutorial\u003c/h2\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488309881072_-678258458",
      "id": "20170228-202441_2056011249",
      "dateCreated": "Feb 28, 2017 8:24:41 PM",
      "dateStarted": "Mar 1, 2017 9:38:51 AM",
      "dateFinished": "Mar 1, 2017 9:38:51 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark \nprintln(sc.version)\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 10:41:39 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "2.1.0\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488361279208_1039337105",
      "id": "20170301-104119_326642504",
      "dateCreated": "Mar 1, 2017 10:41:19 AM",
      "dateStarted": "Mar 1, 2017 10:41:39 AM",
      "dateFinished": "Mar 1, 2017 10:41:42 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh \nls -lt /Users/juantomas/proyectos/cursos/curso-spark/datasets\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 9:38:56 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "total 1347920\n-rw-r--r--@ 1 juantomas  staff  689413344 28 feb 22:07 2008.csv\n-rw-r--r--@ 1 juantomas  staff     244438 28 feb 22:07 airports.csv\n-rw-r--r--@ 1 juantomas  staff      43758 28 feb 22:07 carriers.csv\n-rw-r--r--@ 1 juantomas  staff     428796 28 feb 22:07 plane-data.csv\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488311797987_-595394070",
      "id": "20170228-205637_1262468898",
      "dateCreated": "Feb 28, 2017 8:56:37 PM",
      "dateStarted": "Mar 1, 2017 9:38:57 AM",
      "dateFinished": "Mar 1, 2017 9:38:58 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n**USA Flights 2008**\n\nhttp://stat-computing.org/dataexpo/2009/2008.csv.bz2\n\n**Github**\n\nhttps://github.com/juantomasgarcia/curso-spark\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 9:54:15 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003e\u003cstrong\u003eUSA Flights 2008\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href\u003d\"http://stat-computing.org/dataexpo/2009/2008.csv.bz2\"\u003ehttp://stat-computing.org/dataexpo/2009/2008.csv.bz2\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eGithub\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href\u003d\"https://github.com/juantomasgarcia/curso-spark\"\u003ehttps://github.com/juantomasgarcia/curso-spark\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488316346559_615287004",
      "id": "20170228-221226_181477098",
      "dateCreated": "Feb 28, 2017 10:12:26 PM",
      "dateStarted": "Mar 1, 2017 9:54:15 AM",
      "dateFinished": "Mar 1, 2017 9:54:15 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nFirst Example (for the impacients)\n---\n\n**Step by Step**: we will start just loading a csv file.",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 10:09:00 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eFirst Example (for the impacients)\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eStep by Step\u003c/strong\u003e: we will start just loading a csv file.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488316958112_1774440206",
      "id": "20170228-222238_268250743",
      "dateCreated": "Feb 28, 2017 10:22:38 PM",
      "dateStarted": "Mar 1, 2017 10:09:00 AM",
      "dateFinished": "Mar 1, 2017 10:09:00 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val localPath\u003d\"/Users/juantomas/proyectos/cursos/curso-spark/datasets/\"\nval airports \u003d sqlContext.read.format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .load(localPath+\"airports.csv\")\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 10:32:19 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nlocalPath: String \u003d /Users/juantomas/proyectos/cursos/curso-spark/datasets/\n\nairports: org.apache.spark.sql.DataFrame \u003d [iata: string, airport: string ... 5 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488309788667_679951132",
      "id": "20170228-202308_1487435516",
      "dateCreated": "Feb 28, 2017 8:23:08 PM",
      "dateStarted": "Feb 28, 2017 10:24:19 PM",
      "dateFinished": "Feb 28, 2017 10:24:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nWe just create a DataFrame reading the file \nThe schema of the Dataframe is: \n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:28:51 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eWe just create a DataFrame reading the file\u003cbr/\u003eThe schema of the Dataframe is:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488317237175_1793291258",
      "id": "20170228-222717_1840369942",
      "dateCreated": "Feb 28, 2017 10:27:17 PM",
      "dateStarted": "Feb 28, 2017 10:28:51 PM",
      "dateFinished": "Feb 28, 2017 10:28:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nairports.printSchema\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:26:27 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- iata: string (nullable \u003d true)\n |-- airport: string (nullable \u003d true)\n |-- city: string (nullable \u003d true)\n |-- state: string (nullable \u003d true)\n |-- country: string (nullable \u003d true)\n |-- lat: double (nullable \u003d true)\n |-- long: double (nullable \u003d true)\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488317095679_-1440385330",
      "id": "20170228-222455_1852965182",
      "dateCreated": "Feb 28, 2017 10:24:55 PM",
      "dateStarted": "Feb 28, 2017 10:26:28 PM",
      "dateFinished": "Feb 28, 2017 10:26:29 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nThe contents of the dataframe airports is: \n\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:29:48 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eThe contents of the dataframe airports is:\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488317225593_2090286760",
      "id": "20170228-222705_478435638",
      "dateCreated": "Feb 28, 2017 10:27:05 PM",
      "dateStarted": "Feb 28, 2017 10:29:48 PM",
      "dateFinished": "Feb 28, 2017 10:29:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n\nairports.show(10)\nprintln(\"Total Airports: \" + airports.count.toString)",
      "user": "anonymous",
      "dateUpdated": "Mar 2, 2017 10:15:04 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+----+--------------------+----------------+-----+-------+-----------+------------+\n|iata|             airport|            city|state|country|        lat|        long|\n+----+--------------------+----------------+-----+-------+-----------+------------+\n| 00M|            Thigpen |     Bay Springs|   MS|    USA|31.95376472|-89.23450472|\n| 00R|Livingston Municipal|      Livingston|   TX|    USA|30.68586111|-95.01792778|\n| 00V|         Meadow Lake|Colorado Springs|   CO|    USA|38.94574889|-104.5698933|\n| 01G|        Perry-Warsaw|           Perry|   NY|    USA|42.74134667|-78.05208056|\n| 01J|    Hilliard Airpark|        Hilliard|   FL|    USA| 30.6880125|-81.90594389|\n| 01M|   Tishomingo County|         Belmont|   MS|    USA|34.49166667|-88.20111111|\n| 02A|         Gragg-Wade |         Clanton|   AL|    USA|32.85048667|-86.61145333|\n| 02C|             Capitol|      Brookfield|   WI|    USA|   43.08751|-88.17786917|\n| 02G|   Columbiana County|  East Liverpool|   OH|    USA|40.67331278|-80.64140639|\n| 03D|    Memphis Memorial|         Memphis|   MO|    USA|40.44725889|-92.22696056|\n+----+--------------------+----------------+-----+-------+-----------+------------+\nonly showing top 10 rows\n\nTotal Airports: 3376\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488316843497_847394571",
      "id": "20170228-222043_2046966645",
      "dateCreated": "Feb 28, 2017 10:20:43 PM",
      "dateStarted": "Mar 2, 2017 10:15:04 AM",
      "dateFinished": "Mar 2, 2017 10:42:18 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nairports.registerTempTable(\"airports\")",
      "user": "anonymous",
      "dateUpdated": "Mar 2, 2017 10:09:19 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488445729185_737710356",
      "id": "20170302-100849_130432828",
      "dateCreated": "Mar 2, 2017 10:08:49 AM",
      "dateStarted": "Mar 2, 2017 10:09:20 AM",
      "dateFinished": "Mar 2, 2017 10:09:25 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql \nselect state, count(1) from airports group by state\n",
      "user": "anonymous",
      "dateUpdated": "Mar 2, 2017 10:14:52 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sql"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "state\tcount(1)\nAZ\t59\nSC\t52\nLA\t55\nMN\t89\nNJ\t35\nDC\t1\nOR\t57\nNA\t12\nVA\t47\nRI\t6\nKY\t50\nWY\t32\nNH\t14\nMI\t94\nNV\t32\nWI\t84\nID\t37\nCA\t205\nNE\t73\nCT\t15\nMT\t71\nNC\t72\nVT\t13\nMD\t18\nDE\t5\nMO\t74\nVI\t5\nIL\t88\nME\t34\nGU\t1\nND\t52\nWA\t65\nMS\t72\nAL\t73\nIN\t65\nOH\t100\nTN\t70\nNM\t51\nIA\t78\nPA\t71\nSD\t57\nNY\t97\nTX\t209\nWV\t24\nGA\t97\nMA\t30\nAS\t3\nKS\t78\nCO\t49\nFL\t100\nAK\t263\nAR\t74\nOK\t102\nPR\t11\nCQ\t4\nUT\t35\nHI\t16\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488445775565_2061074717",
      "id": "20170302-100935_276568115",
      "dateCreated": "Mar 2, 2017 10:09:35 AM",
      "dateStarted": "Mar 2, 2017 10:14:53 AM",
      "dateFinished": "Mar 2, 2017 10:42:16 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nval flights \u003d sqlContext.read.format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .load(localPath+\"2008.csv\")\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:40:53 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nflights: org.apache.spark.sql.DataFrame \u003d [Year: int, Month: int ... 27 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488317167429_-1582434295",
      "id": "20170228-222607_1793357259",
      "dateCreated": "Feb 28, 2017 10:26:07 PM",
      "dateStarted": "Feb 28, 2017 10:40:53 PM",
      "dateFinished": "Feb 28, 2017 10:41:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nflights.printSchema\nflights.show(10)\nprintln(\"Toatal: \" + flights.count)\n\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 10:43:33 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 304.0,
              "optionOpen": false
            }
          }
        },
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "root\n |-- Year: integer (nullable \u003d true)\n |-- Month: integer (nullable \u003d true)\n |-- DayofMonth: integer (nullable \u003d true)\n |-- DayOfWeek: integer (nullable \u003d true)\n |-- DepTime: string (nullable \u003d true)\n |-- CRSDepTime: integer (nullable \u003d true)\n |-- ArrTime: string (nullable \u003d true)\n |-- CRSArrTime: integer (nullable \u003d true)\n |-- UniqueCarrier: string (nullable \u003d true)\n |-- FlightNum: integer (nullable \u003d true)\n |-- TailNum: string (nullable \u003d true)\n |-- ActualElapsedTime: string (nullable \u003d true)\n |-- CRSElapsedTime: string (nullable \u003d true)\n |-- AirTime: string (nullable \u003d true)\n |-- ArrDelay: string (nullable \u003d true)\n |-- DepDelay: string (nullable \u003d true)\n |-- Origin: string (nullable \u003d true)\n |-- Dest: string (nullable \u003d true)\n |-- Distance: integer (nullable \u003d true)\n |-- TaxiIn: string (nullable \u003d true)\n |-- TaxiOut: string (nullable \u003d true)\n |-- Cancelled: integer (nullable \u003d true)\n |-- CancellationCode: string (nullable \u003d true)\n |-- Diverted: integer (nullable \u003d true)\n |-- CarrierDelay: string (nullable \u003d true)\n |-- WeatherDelay: string (nullable \u003d true)\n |-- NASDelay: string (nullable \u003d true)\n |-- SecurityDelay: string (nullable \u003d true)\n |-- LateAircraftDelay: string (nullable \u003d true)\n\n+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n|2008|    1|         3|        4|   2003|      1955|   2211|      2225|           WN|      335| N712SW|              128|           150|    116|     -14|       8|   IAD| TPA|     810|     4|      8|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n|2008|    1|         3|        4|    754|       735|   1002|      1000|           WN|     3231| N772SW|              128|           145|    113|       2|      19|   IAD| TPA|     810|     5|     10|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n|2008|    1|         3|        4|    628|       620|    804|       750|           WN|      448| N428WN|               96|            90|     76|      14|       8|   IND| BWI|     515|     3|     17|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n|2008|    1|         3|        4|    926|       930|   1054|      1100|           WN|     1746| N612SW|               88|            90|     78|      -6|      -4|   IND| BWI|     515|     3|      7|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n|2008|    1|         3|        4|   1829|      1755|   1959|      1925|           WN|     3920| N464WN|               90|            90|     77|      34|      34|   IND| BWI|     515|     3|     10|        0|            null|       0|           2|           0|       0|            0|               32|\n|2008|    1|         3|        4|   1940|      1915|   2121|      2110|           WN|      378| N726SW|              101|           115|     87|      11|      25|   IND| JAX|     688|     4|     10|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n|2008|    1|         3|        4|   1937|      1830|   2037|      1940|           WN|      509| N763SW|              240|           250|    230|      57|      67|   IND| LAS|    1591|     3|      7|        0|            null|       0|          10|           0|       0|            0|               47|\n|2008|    1|         3|        4|   1039|      1040|   1132|      1150|           WN|      535| N428WN|              233|           250|    219|     -18|      -1|   IND| LAS|    1591|     7|      7|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n|2008|    1|         3|        4|    617|       615|    652|       650|           WN|       11| N689SW|               95|            95|     70|       2|       2|   IND| MCI|     451|     6|     19|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n|2008|    1|         3|        4|   1620|      1620|   1639|      1655|           WN|      810| N648SW|               79|            95|     70|     -16|       0|   IND| MCI|     451|     3|      6|        0|            null|       0|          NA|          NA|      NA|           NA|               NA|\n+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\nonly showing top 10 rows\n\nToatal: 7009728\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488318053270_1234204013",
      "id": "20170228-224053_1830699528",
      "dateCreated": "Feb 28, 2017 10:40:53 PM",
      "dateStarted": "Feb 28, 2017 10:42:36 PM",
      "dateFinished": "Feb 28, 2017 10:42:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n\nSo what is Spark SQL? \n---\n\n\n* At the very start was a derivate of HIVEQL \n* Spark SQL is a Spark library that runs on top of Spark\n* It provides a higher-level abstraction than the Spark core API for processing structured data\n* Structured data includes data stored in a database, NoSQL data store, Parquet, ORC, Avro, JSON, CSV, or any other structured format\n* The NoSQL data stores that can be used with Spark SQL include HBase, Cassandra, Elasticsearch, Druid, and other NoSQL data stores\n* Spark SQL is more than just about providing SQL interface to Spark\n* Spark SQL can be used as a library for developing data processing applications in Scala, Java, Python, or R\n* internally uses the Spark core API to execute queries on a Spark cluster\n* Spark SQL seamlessly integrates with other Spark libraries such as Spark Streaming, Spark ML, and GraphX\n\n",
      "user": "anonymous",
      "dateUpdated": "Feb 28, 2017 11:37:54 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSo what is Spark SQL?\u003c/h2\u003e\n\u003cul\u003e\n  \u003cli\u003eAt the very start was a derivate of HIVEQL\u003c/li\u003e\n  \u003cli\u003eSpark SQL is a Spark library that runs on top of Spark\u003c/li\u003e\n  \u003cli\u003eIt provides a higher-level abstraction than the Spark core API for processing structured data\u003c/li\u003e\n  \u003cli\u003eStructured data includes data stored in a database, NoSQL data store, Parquet, ORC, Avro, JSON, CSV, or any other structured format\u003c/li\u003e\n  \u003cli\u003eThe NoSQL data stores that can be used with Spark SQL include HBase, Cassandra, Elasticsearch, Druid, and other NoSQL data stores\u003c/li\u003e\n  \u003cli\u003eSpark SQL is more than just about providing SQL interface to Spark\u003c/li\u003e\n  \u003cli\u003eSpark SQL can be used as a library for developing data processing applications in Scala, Java, Python, or R\u003c/li\u003e\n  \u003cli\u003einternally uses the Spark core API to execute queries on a Spark cluster\u003c/li\u003e\n  \u003cli\u003eSpark SQL seamlessly integrates with other Spark libraries such as Spark Streaming, Spark ML, and GraphX\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488318155881_277638337",
      "id": "20170228-224235_216639834",
      "dateCreated": "Feb 28, 2017 10:42:35 PM",
      "dateStarted": "Feb 28, 2017 11:37:54 PM",
      "dateFinished": "Feb 28, 2017 11:37:54 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nThat\u0027s the global view of spark \u0026 spark SQL \n---\n\n![alternate text](https://www.safaribooksonline.com/library/view/big-data-analytics/9781484209646/images/9781484209653_Fig07-01.jpg)\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 2, 2017 6:18:13 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eThat\u0026rsquo;s the global view of spark \u0026amp; spark SQL\u003c/h2\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://www.safaribooksonline.com/library/view/big-data-analytics/9781484209646/images/9781484209653_Fig07-01.jpg\" alt\u003d\"alternate text\" /\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488318804163_-1162528226",
      "id": "20170228-225324_353156368",
      "dateCreated": "Feb 28, 2017 10:53:24 PM",
      "dateStarted": "Mar 1, 2017 9:39:25 AM",
      "dateFinished": "Mar 1, 2017 9:39:25 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n\nKeypoints of Spark SQL \n---\n\n**Performance**\n\n\n*TL;DR*: Spark SQL makes data processing applications run faster using a combination of techniques, including reduced disk I/O, in-memory columnar caching, query optimization, and code generation.\n\n\n* Reduced Disk I/O\n    * Disk I/O is slow\n    * can skip non-required partitions, rows, or columns while reading data.\n\n* Partitioning\n    * a lot of I/O can be avoided by partitioning a dataset\n    \n* Columnar Storage\n    * Spark SQL supports columnar storage formats such as Parquet, which allow reading of only the columns that are used in a query\n\n* In-Memory Columnar Caching\n    * Spark SQL allows an application to cache data in an in-memory columnar format from any data source\n    * Spark SQL, caches only the required columns\n    * Spark SQL compresses the cached columns to minimize memory usage and JVM garbage collection pressure\n    * apply efficient compression techniques such as run length encoding, delta encoding, and dictionary encoding\n    \n* Skip Rows\n    * If a data source maintains statistical information about a dataset, Spark SQL takes advantage of it\n    * Serialization formats such as Parquet and ORC store min and max values for each column in a row group or chunk of rows\n\n* Predicate Pushdown\n    * Instead of reading an entire table and then executing a filtering operation, Spark SQL will ask the database to natively execute the filtering operation\n    * Since databases generally index data, native filtering is much faster than filtering at the application layer\n    \n* Query Optimization\n    * analysis\n    * logical optimization\n    * physical planning\n    * code generation\n\n* Applications \n    * Spark SQL can be used as a ETL\n    * Very fast conversions between formats.\n    * Few lines of code.\n    * You can even perform join operations across different data sources.\n\n* Distributed JDBC/ODBC SQL Query Engine\n    * can be used as a library. In this mode, data processing tasks can be expressed as SQL, HiveQL or language integrated queries within a Scala, Java, Python, or R application\n    * It comes prepackaged with a Thrift/JDBC/ODBC server\n    \n* Data Warehousing\n    * Spark SQL store and analyze large amounts of data like a warehouse\n    * Spark SQL-based data warehousing solution is more scalable, economical, and flexible than the proprietary data warehousing solutions\n    * is flexible since it supports both schema-on-read and schema-on-write",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:18:31 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eKeypoints of Spark SQL\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003ePerformance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eTL;DR\u003c/em\u003e: Spark SQL makes data processing applications run faster using a combination of techniques, including reduced disk I/O, in-memory columnar caching, query optimization, and code generation.\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eReduced Disk I/O\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eDisk I/O is slow\u003c/li\u003e\n      \u003cli\u003ecan skip non-required partitions, rows, or columns while reading data.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003ePartitioning\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003ea lot of I/O can be avoided by partitioning a dataset\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eColumnar Storage\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eSpark SQL supports columnar storage formats such as Parquet, which allow reading of only the columns that are used in a query\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eIn-Memory Columnar Caching\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eSpark SQL allows an application to cache data in an in-memory columnar format from any data source\u003c/li\u003e\n      \u003cli\u003eSpark SQL, caches only the required columns\u003c/li\u003e\n      \u003cli\u003eSpark SQL compresses the cached columns to minimize memory usage and JVM garbage collection pressure\u003c/li\u003e\n      \u003cli\u003eapply efficient compression techniques such as run length encoding, delta encoding, and dictionary encoding\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eSkip Rows\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eIf a data source maintains statistical information about a dataset, Spark SQL takes advantage of it\u003c/li\u003e\n      \u003cli\u003eSerialization formats such as Parquet and ORC store min and max values for each column in a row group or chunk of rows\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003ePredicate Pushdown\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eInstead of reading an entire table and then executing a filtering operation, Spark SQL will ask the database to natively execute the filtering operation\u003c/li\u003e\n      \u003cli\u003eSince databases generally index data, native filtering is much faster than filtering at the application layer\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eQuery Optimization\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eanalysis\u003c/li\u003e\n      \u003cli\u003elogical optimization\u003c/li\u003e\n      \u003cli\u003ephysical planning\u003c/li\u003e\n      \u003cli\u003ecode generation\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eApplications \u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eSpark SQL can be used as a ETL\u003c/li\u003e\n      \u003cli\u003eVery fast conversions between formats.\u003c/li\u003e\n      \u003cli\u003eFew lines of code.\u003c/li\u003e\n      \u003cli\u003eYou can even perform join operations across different data sources.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eDistributed JDBC/ODBC SQL Query Engine\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003ecan be used as a library. In this mode, data processing tasks can be expressed as SQL, HiveQL or language integrated queries within a Scala, Java, Python, or R application\u003c/li\u003e\n      \u003cli\u003eIt comes prepackaged with a Thrift/JDBC/ODBC server\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eData Warehousing\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003eSpark SQL store and analyze large amounts of data like a warehouse\u003c/li\u003e\n      \u003cli\u003eSpark SQL-based data warehousing solution is more scalable, economical, and flexible than the proprietary data warehousing solutions\u003c/li\u003e\n      \u003cli\u003eis flexible since it supports both schema-on-read and schema-on-write\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488321602101_-1180240952",
      "id": "20170228-234002_1917995362",
      "dateCreated": "Feb 28, 2017 11:40:02 PM",
      "dateStarted": "Mar 1, 2017 12:18:31 AM",
      "dateFinished": "Mar 1, 2017 12:18:31 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nApplication Programming Interface (API)\n---\n\nThe Spark SQL API consists of three key abstractions: \n\n* SQLContext\n* HiveContext \n* DataFrame. \n\nA Spark SQL application processes data using these abstractions.\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:20:32 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eApplication Programming Interface (API)\u003c/h2\u003e\n\u003cp\u003eThe Spark SQL API consists of three key abstractions: \u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eSQLContext\u003c/li\u003e\n  \u003cli\u003eHiveContext\u003c/li\u003e\n  \u003cli\u003eDataFrame.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA Spark SQL application processes data using these abstractions.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488321850550_-605473712",
      "id": "20170228-234410_1366267029",
      "dateCreated": "Feb 28, 2017 11:44:10 PM",
      "dateStarted": "Mar 1, 2017 12:20:32 AM",
      "dateFinished": "Mar 1, 2017 12:20:33 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nSQLContext \n---\n\nSQLContext is the main entry point into the Spark SQL library.\n\nIt is a class defined in the Spark SQL library.\n\nA Spark SQL application must create an instance of the SQLContext or HiveContext class.\n\n**How create it**\n\n~~~\nimport org.apache.spark._\nimport org.apache.spark.sql._\n\nval config \u003d new SparkConf().setAppName(\"My Spark SQL app\")\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new SQLContext(sc)\n\nval resultSet \u003d sqlContext.sql(\"SELECT count(1) FROM my_table\")\n~~~",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:30:32 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "tableHide": false,
        "editorHide": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSQLContext\u003c/h2\u003e\n\u003cp\u003eSQLContext is the main entry point into the Spark SQL library.\u003c/p\u003e\n\u003cp\u003eIt is a class defined in the Spark SQL library.\u003c/p\u003e\n\u003cp\u003eA Spark SQL application must create an instance of the SQLContext or HiveContext class.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHow create it\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark._\nimport org.apache.spark.sql._\n\nval config \u003d new SparkConf().setAppName(\u0026quot;My Spark SQL app\u0026quot;)\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new SQLContext(sc)\n\nval resultSet \u003d sqlContext.sql(\u0026quot;SELECT count(1) FROM my_table\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488324031777_1893317446",
      "id": "20170301-002031_1597575652",
      "dateCreated": "Mar 1, 2017 12:20:31 AM",
      "dateStarted": "Mar 1, 2017 12:30:26 AM",
      "dateFinished": "Mar 1, 2017 12:30:26 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark \n\nsc \nsqlContext\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:25:02 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nres11: org.apache.spark.SparkContext \u003d org.apache.spark.SparkContext@3a63e394\n\nres12: org.apache.spark.sql.SQLContext \u003d org.apache.spark.sql.SQLContext@472983b3\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488324247517_-1005305790",
      "id": "20170301-002407_1426558498",
      "dateCreated": "Mar 1, 2017 12:24:07 AM",
      "dateStarted": "Mar 1, 2017 12:25:03 AM",
      "dateFinished": "Mar 1, 2017 12:25:09 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nDataFrames\n---\n\n* DataFrame is Spark SQL’s primary data abstraction\n* a distributed collection of rows organized into named columns.\n* inspired by DataFrames in R and Python\n* similar to a table in a relational database\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:33:13 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eDataFrames\u003c/h2\u003e\n\u003cul\u003e\n  \u003cli\u003eDataFrame is Spark SQL’s primary data abstraction\u003c/li\u003e\n  \u003cli\u003ea distributed collection of rows organized into named columns.\u003c/li\u003e\n  \u003cli\u003einspired by DataFrames in R and Python\u003c/li\u003e\n  \u003cli\u003esimilar to a table in a relational database\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488324302565_688651672",
      "id": "20170301-002502_1565229103",
      "dateCreated": "Mar 1, 2017 12:25:02 AM",
      "dateStarted": "Mar 1, 2017 12:33:13 AM",
      "dateFinished": "Mar 1, 2017 12:33:13 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nDiffs between RDDs \u0026 Dataframes\n---\n\n* DataFrame is schema aware\n* RDD is a partitioned collection of opaque elements\n* DataFrame knows the names and types of the columns in a dataset\n* DataFrame class is able to provide a rich domain-specific-language (DSL) for data processing\n* DataFrame API is easier to understand and use than the RDD API\n* DataFrame can also be operated on as an RDD\n* RDD can be easily created from a DataFrame\n* DataFrame can be registered as a temporary table\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 11:56:01 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eDiffs between RDDs \u0026amp; Dataframes\u003c/h2\u003e\n\u003cul\u003e\n  \u003cli\u003eDataFrame is schema aware\u003c/li\u003e\n  \u003cli\u003eRDD is a partitioned collection of opaque elements\u003c/li\u003e\n  \u003cli\u003eDataFrame knows the names and types of the columns in a dataset\u003c/li\u003e\n  \u003cli\u003eDataFrame class is able to provide a rich domain-specific-language (DSL) for data processing\u003c/li\u003e\n  \u003cli\u003eDataFrame API is easier to understand and use than the RDD API\u003c/li\u003e\n  \u003cli\u003eDataFrame can also be operated on as an RDD\u003c/li\u003e\n  \u003cli\u003eRDD can be easily created from a DataFrame\u003c/li\u003e\n  \u003cli\u003eDataFrame can be registered as a temporary table\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488324777727_-1194054392",
      "id": "20170301-003257_151015170",
      "dateCreated": "Mar 1, 2017 12:32:57 AM",
      "dateStarted": "Mar 1, 2017 11:56:01 AM",
      "dateFinished": "Mar 1, 2017 11:56:01 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nRow\n---\n\nRow is a Spark SQL abstraction for representing a row of data\n\nit is equivalent to a relational tuple or row in a table\n\nYou can create a Row this way: \n\n~~~\nimport org.apache.spark.sql._\n\nval row1 \u003d Row(\"Barack Obama\", \"President\", \"United States\")\nval row2 \u003d Row(\"David Cameron\", \"Prime Minister\", \"United Kingdom\")\n~~~\n\nand you can use this rows at low level: \n\n~~~\nval presidentName \u003d row1.getString(0)\nval country \u003d row1.getString(2)\n~~~",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:02:05 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eRow\u003c/h2\u003e\n\u003cp\u003eRow is a Spark SQL abstraction for representing a row of data\u003c/p\u003e\n\u003cp\u003eit is equivalent to a relational tuple or row in a table\u003c/p\u003e\n\u003cp\u003eYou can create a Row this way: \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark.sql._\n\nval row1 \u003d Row(\u0026quot;Barack Obama\u0026quot;, \u0026quot;President\u0026quot;, \u0026quot;United States\u0026quot;)\nval row2 \u003d Row(\u0026quot;David Cameron\u0026quot;, \u0026quot;Prime Minister\u0026quot;, \u0026quot;United Kingdom\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand you can use this rows at low level: \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval presidentName \u003d row1.getString(0)\nval country \u003d row1.getString(2)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488325042529_1230631532",
      "id": "20170301-003722_1470503227",
      "dateCreated": "Mar 1, 2017 12:37:22 AM",
      "dateStarted": "Mar 1, 2017 12:02:05 PM",
      "dateFinished": "Mar 1, 2017 12:02:05 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nHow to create a Dataframe: \n---\n\n**a) from an RDD** \n\n~~~\nimport org.apache.spark._\nimport org.apache.spark.sql._\n\nval config \u003d new SparkConf().setAppName(\"My Spark SQL app\")\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new SQLContext(sc)\nimport sqlContext.implicits._\n\ncase class Employee(name: String, age: Int, gender: String)\n\nval rowsRDD \u003d sc.textFile(\"path/to/employees.csv\")\nval employeesRDD \u003d rowsRDD.map{row \u003d\u003e row.split(\",\")}\n                          .map{cols \u003d\u003e Employee(cols(0), cols(1).trim.toInt, cols(2))}\n\nval employeesDF \u003d employeesRDD.toDF()\n\n~~~\n\n**b) createDataFrame**\n\n~~~\nimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\n\nval config \u003d new SparkConf().setAppName(\"My Spark SQL app\")\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new SQLContext(sc)\n\nval linesRDD \u003d sc.textFile(\"path/to/employees.csv\")\nval rowsRDD \u003d linesRDD.map{row \u003d\u003e row.split(\",\")}\n                      .map{cols \u003d\u003e Row(cols(0), cols(1).trim.toInt, cols(2))}\n\nval schema \u003d StructType(List(\n                 StructField(\"name\", StringType, false),\n                 StructField(\"age\", IntegerType, false),\n                 StructField(\"gender\", StringType, false)\n               )\n             )\n\nval employeesDF \u003d sqlContext.createDataFrame(rowsRDD,schema)\n~~~\n\n**c) Creating a DataFrame from a Data Source**\n\nSpark SQL provides a unified interface for creating a DataFrame from a variety of data sources.\n\nAPI can be used to create a DataFrame from a MySQL, PostgreSQL, Oracle, or Cassandra table.\n\nthe same API can be used to create a DataFrame from a Parquet, JSON, ORC or CSV file on local file system, HDFS or S3\n\nSpark SQL has built-in support for some of the commonly used data sources\n\nIncludes Parquet, JSON, Hive, and JDBC-compliant databases\n\nExternal packages are available for other data sources\n\n~~~\nimport org.apache.spark._\nimport org.apache.spark.sql._\n\nval config \u003d new SparkConf().setAppName(\"My Spark SQL app\")\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new org.apache.spark.sql.hive.HiveContext (sc)\n\n// create a DataFrame from parquet files\nval parquetDF \u003d sqlContext.read\n                          .format(\"org.apache.spark.sql.parquet\")\n                          .load(\"path/to/Parquet-file-or-directory\")\n\n// create a DataFrame from JSON files\nval jsonDF \u003d sqlContext.read\n                       .format(\"org.apache.spark.sql.json\")\n                       .load(\"path/to/JSON-file-or-directory\")\n\n// create a DataFrame from a table in a Postgres database\nval jdbcDF \u003d sqlContext.read\n               .format(\"org.apache.spark.sql.jdbc\")\n               .options(Map(\n                  \"url\" -\u003e \"jdbc:postgresql://host:port/database?user\u003d\u003cUSER\u003e\u0026password\u003d\u003cPASS\u003e\",\n                  \"dbtable\" -\u003e \"schema-name.table-name\"))\n               .load()\n\n// create a DataFrame from a Hive table\nval hiveDF \u003d sqlContext.read\n                       .table(\"hive-table-name\")\n~~~\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:50:51 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eHow to create a Dataframe:\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003ea) from an RDD\u003c/strong\u003e \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark._\nimport org.apache.spark.sql._\n\nval config \u003d new SparkConf().setAppName(\u0026quot;My Spark SQL app\u0026quot;)\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new SQLContext(sc)\nimport sqlContext.implicits._\n\ncase class Employee(name: String, age: Int, gender: String)\n\nval rowsRDD \u003d sc.textFile(\u0026quot;path/to/employees.csv\u0026quot;)\nval employeesRDD \u003d rowsRDD.map{row \u003d\u0026gt; row.split(\u0026quot;,\u0026quot;)}\n                          .map{cols \u003d\u0026gt; Employee(cols(0), cols(1).trim.toInt, cols(2))}\n\nval employeesDF \u003d employeesRDD.toDF()\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eb) createDataFrame\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\n\nval config \u003d new SparkConf().setAppName(\u0026quot;My Spark SQL app\u0026quot;)\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new SQLContext(sc)\n\nval linesRDD \u003d sc.textFile(\u0026quot;path/to/employees.csv\u0026quot;)\nval rowsRDD \u003d linesRDD.map{row \u003d\u0026gt; row.split(\u0026quot;,\u0026quot;)}\n                      .map{cols \u003d\u0026gt; Row(cols(0), cols(1).trim.toInt, cols(2))}\n\nval schema \u003d StructType(List(\n                 StructField(\u0026quot;name\u0026quot;, StringType, false),\n                 StructField(\u0026quot;age\u0026quot;, IntegerType, false),\n                 StructField(\u0026quot;gender\u0026quot;, StringType, false)\n               )\n             )\n\nval employeesDF \u003d sqlContext.createDataFrame(rowsRDD,schema)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003ec) Creating a DataFrame from a Data Source\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpark SQL provides a unified interface for creating a DataFrame from a variety of data sources.\u003c/p\u003e\n\u003cp\u003eAPI can be used to create a DataFrame from a MySQL, PostgreSQL, Oracle, or Cassandra table.\u003c/p\u003e\n\u003cp\u003ethe same API can be used to create a DataFrame from a Parquet, JSON, ORC or CSV file on local file system, HDFS or S3\u003c/p\u003e\n\u003cp\u003eSpark SQL has built-in support for some of the commonly used data sources\u003c/p\u003e\n\u003cp\u003eIncludes Parquet, JSON, Hive, and JDBC-compliant databases\u003c/p\u003e\n\u003cp\u003eExternal packages are available for other data sources\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark._\nimport org.apache.spark.sql._\n\nval config \u003d new SparkConf().setAppName(\u0026quot;My Spark SQL app\u0026quot;)\nval sc \u003d new SparkContext(config)\nval sqlContext \u003d new org.apache.spark.sql.hive.HiveContext (sc)\n\n// create a DataFrame from parquet files\nval parquetDF \u003d sqlContext.read\n                          .format(\u0026quot;org.apache.spark.sql.parquet\u0026quot;)\n                          .load(\u0026quot;path/to/Parquet-file-or-directory\u0026quot;)\n\n// create a DataFrame from JSON files\nval jsonDF \u003d sqlContext.read\n                       .format(\u0026quot;org.apache.spark.sql.json\u0026quot;)\n                       .load(\u0026quot;path/to/JSON-file-or-directory\u0026quot;)\n\n// create a DataFrame from a table in a Postgres database\nval jdbcDF \u003d sqlContext.read\n               .format(\u0026quot;org.apache.spark.sql.jdbc\u0026quot;)\n               .options(Map(\n                  \u0026quot;url\u0026quot; -\u0026gt; \u0026quot;jdbc:postgresql://host:port/database?user\u003d\u0026lt;USER\u0026gt;\u0026amp;password\u003d\u0026lt;PASS\u0026gt;\u0026quot;,\n                  \u0026quot;dbtable\u0026quot; -\u0026gt; \u0026quot;schema-name.table-name\u0026quot;))\n               .load()\n\n// create a DataFrame from a Hive table\nval hiveDF \u003d sqlContext.read\n                       .table(\u0026quot;hive-table-name\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488325207232_-997820417",
      "id": "20170301-004007_185660401",
      "dateCreated": "Mar 1, 2017 12:40:07 AM",
      "dateStarted": "Mar 1, 2017 12:50:52 AM",
      "dateFinished": "Mar 1, 2017 12:50:52 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nSome examples reading from different sources \n---\n\n**JSON**\n\n~~~\nval jsonDF \u003d sqlContext.read.json(\"path/to/JSON-file-or-directory\")\nval jsonHdfsDF \u003d sqlContext.read.json(\"hdfs://NAME_NODE/path/to/data.json\")\nval jsonS3DF \u003d sqlContext.read.json(\"s3a://BUCKET_NAME/FOLDER_NAME/data.json\")\n~~~\nPassing the schema to dataframe reader: \n~~~\nimport org.apache.spark.sql.types._\n\nval userSchema \u003d StructType(List(\n                     StructField(\"name\", StringType, false),\n                     StructField(\"age\", IntegerType, false),\n                     StructField(\"gender\", StringType, false)\n                     )\n                   )\n\nval userDF \u003d sqlContext.read\n                       .schema(userSchema)\n                       .json(\"path/to/user.json\")\n~~~\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:15:51 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSome examples reading from different sources\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eJSON\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval jsonDF \u003d sqlContext.read.json(\u0026quot;path/to/JSON-file-or-directory\u0026quot;)\nval jsonHdfsDF \u003d sqlContext.read.json(\u0026quot;hdfs://NAME_NODE/path/to/data.json\u0026quot;)\nval jsonS3DF \u003d sqlContext.read.json(\u0026quot;s3a://BUCKET_NAME/FOLDER_NAME/data.json\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePassing the schema to dataframe reader: \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark.sql.types._\n\nval userSchema \u003d StructType(List(\n                     StructField(\u0026quot;name\u0026quot;, StringType, false),\n                     StructField(\u0026quot;age\u0026quot;, IntegerType, false),\n                     StructField(\u0026quot;gender\u0026quot;, StringType, false)\n                     )\n                   )\n\nval userDF \u003d sqlContext.read\n                       .schema(userSchema)\n                       .json(\u0026quot;path/to/user.json\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488325610148_-2044538284",
      "id": "20170301-004650_654668507",
      "dateCreated": "Mar 1, 2017 12:46:50 AM",
      "dateStarted": "Mar 1, 2017 12:15:51 PM",
      "dateFinished": "Mar 1, 2017 12:15:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n**Parquet**\n~~~\nval parquetDF \u003d sqlContext.read.parquet(\"path/to/parquet-file-or-directory\")\n~~~\n\n**ORC**\n\n~~~\nval orcDF \u003d hiveContext.read.orc(\"path/to/orc-file-or-directory\")\n~~~\n\n**Hive**\n\n~~~\nval hiveDF \u003d hiveContext.read.table(\"hive-table-name\")\nval hiveDF \u003d hiveContext.sql(\"SELECT col_a, col_b, col_c from hive-table\")\n~~~\n\n**JDBC**\n\n~~~\nval jdbcUrl \u003d\"jdbc:mysql://host:port/database\"\nval tableName \u003d \"table-name\"\nval connectionProperties \u003d new java.util.Properties\nconnectionProperties.setProperty(\"user\",\"database-user-name\")\nconnectionProperties.setProperty(\"password\",\" database-user-password\")\n\nval jdbcDF \u003d hiveContext.read\n                        .jdbc(jdbcUrl, tableName, connectionProperties)\n~~~\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:59:48 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003e\u003cstrong\u003eParquet\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval parquetDF \u003d sqlContext.read.parquet(\u0026quot;path/to/parquet-file-or-directory\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eORC\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval orcDF \u003d hiveContext.read.orc(\u0026quot;path/to/orc-file-or-directory\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eHive\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval hiveDF \u003d hiveContext.read.table(\u0026quot;hive-table-name\u0026quot;)\nval hiveDF \u003d hiveContext.sql(\u0026quot;SELECT col_a, col_b, col_c from hive-table\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eJDBC\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval jdbcUrl \u003d\u0026quot;jdbc:mysql://host:port/database\u0026quot;\nval tableName \u003d \u0026quot;table-name\u0026quot;\nval connectionProperties \u003d new java.util.Properties\nconnectionProperties.setProperty(\u0026quot;user\u0026quot;,\u0026quot;database-user-name\u0026quot;)\nconnectionProperties.setProperty(\u0026quot;password\u0026quot;,\u0026quot; database-user-password\u0026quot;)\n\nval jdbcDF \u003d hiveContext.read\n                        .jdbc(jdbcUrl, tableName, connectionProperties)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488326081842_1797425193",
      "id": "20170301-005441_1241995655",
      "dateCreated": "Mar 1, 2017 12:54:41 AM",
      "dateStarted": "Mar 1, 2017 12:59:41 AM",
      "dateFinished": "Mar 1, 2017 12:59:41 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nBasic Operations\n---\n\n**cache**\n\nThe cache method stores the source DataFrame in memory using a columnar format. It scans only the required columns and stores them in compressed in-memory columnar format. Spark SQL automatically selects a compression codec for each column based on data statistics.\n\n~~~\ncustomerDF.cache()\n~~~\n\nThe caching functionality can be tuned using the setConf method in the SQLContext or HiveContext class. The two configuration parameters for caching are spark.sql.inMemoryColumnarStorage.compressed and spark.sql.inMemoryColumnarStorage.batchSize. By default, compression is turned on and the batch size for columnar caching is 10,000.\n~~~\nsqlContext.setConf(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\")\nsqlContext.setConf(\"spark.sql.inMemoryColumnarStorage.batchSize\", \"10000\")\n~~~\n\n**columns**\n\nThe columns method returns the names of all the columns in the source DataFrame as an array of String.\n~~~\nval cols \u003d customerDF.columns\n\ncols: Array[String] \u003d Array(cId, name, age, gender)\n~~~\n\n**dtypes**\n\nThe dtypes method returns the data types of all the columns in the source DataFrame as an array of tuples. The first element in a tuple is the name of a column and the second element is the data type of that column.\n\n~~~\nval columnsWithTypes \u003d customerDF.dtypes\n\ncolumnsWithTypes: Array[(String, String)] \u003d Array((cId,LongType), (name,StringType), (age,IntegerType), (gender,StringType))\n~~~\n\n**explain**\n\nThe explain method prints the physical plan on the console. It is useful for debugging.\n~~~\ncustomerDF.explain()\n\n\u003d\u003d Physical Plan \u003d\u003d\nInMemoryColumnarTableScan [cId#0L,name#1,age#2,gender#3], (InMemoryRelation [cId#0L,name#1,age#2,gender#3], true, 10000, StorageLevel(true, true, false, true, 1), (Scan PhysicalRDD[cId#0L,name#1,age#2,gender#3]), None)\n~~~\n\nA variant of the explain method takes a Boolean argument and prints both logical and physical plans if the argument is true.\n\n**persist**\n\nThe persist method caches the source DataFrame in memory.\n\n~~~\ncustomerDF.persist\n~~~\nSimilar to the persist method in the RDD class, a variant of the persist method in the DataFrame class allows you to specify the storage level for a persisted DataFrame.\n\n**printSchema**\n\nThe printSchema method prints the schema of the source DataFrame on the console in a tree format.\n~~~\ncustomerDF.printSchema()\n\nroot\n |-- cId: long (nullable \u003d false)\n |-- name: string (nullable \u003d true)\n |-- age: integer (nullable \u003d false)\n |-- gender: string (nullable \u003d true)\n~~~\n\n**registerTempTable**\n\nThe registerTempTable method creates a temporary table in Hive metastore. It takes a table name as an argument.\n\nA temporary table can be queried using the sql method in SQLContext or HiveContext. It is available only during the lifespan of the application that creates it.\n~~~\ncustomerDF.registerTempTable(\"customer\")\nval countDF \u003d sqlContext.sql(\"SELECT count(1) AS cnt FROM customer\")\n\ncountDF: org.apache.spark.sql.DataFrame \u003d [cnt: bigint]\n~~~\nNote that the sql method returns a DataFrame. Let’s discuss how to view or retrieve the contents of a DataFrame in a moment.\n\n**toDF**\n\nThe toDF method allows you to rename the columns in the source DataFrame. It takes the new names of the columns as arguments and returns a new DataFrame.\n~~~\nval resultDF \u003d sqlContext.sql(\"SELECT count(1) from customer\")\n\nresultDF: org.apache.spark.sql.DataFrame \u003d [_c0: bigint]\n\nval countDF \u003d resultDF.toDF(\"cnt\")\n\ncountDF: org.apache.spark.sql.DataFrame \u003d [cnt: bigint]\n\n~~~\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 1:25:11 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eBasic Operations\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003ecache\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe cache method stores the source DataFrame in memory using a columnar format. It scans only the required columns and stores them in compressed in-memory columnar format. Spark SQL automatically selects a compression codec for each column based on data statistics.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.cache()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe caching functionality can be tuned using the setConf method in the SQLContext or HiveContext class. The two configuration parameters for caching are spark.sql.inMemoryColumnarStorage.compressed and spark.sql.inMemoryColumnarStorage.batchSize. By default, compression is turned on and the batch size for columnar caching is 10,000.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.setConf(\u0026quot;spark.sql.inMemoryColumnarStorage.compressed\u0026quot;, \u0026quot;true\u0026quot;)\nsqlContext.setConf(\u0026quot;spark.sql.inMemoryColumnarStorage.batchSize\u0026quot;, \u0026quot;10000\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003ecolumns\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe columns method returns the names of all the columns in the source DataFrame as an array of String.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval cols \u003d customerDF.columns\n\ncols: Array[String] \u003d Array(cId, name, age, gender)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003edtypes\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe dtypes method returns the data types of all the columns in the source DataFrame as an array of tuples. The first element in a tuple is the name of a column and the second element is the data type of that column.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval columnsWithTypes \u003d customerDF.dtypes\n\ncolumnsWithTypes: Array[(String, String)] \u003d Array((cId,LongType), (name,StringType), (age,IntegerType), (gender,StringType))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eexplain\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe explain method prints the physical plan on the console. It is useful for debugging.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.explain()\n\n\u003d\u003d Physical Plan \u003d\u003d\nInMemoryColumnarTableScan [cId#0L,name#1,age#2,gender#3], (InMemoryRelation [cId#0L,name#1,age#2,gender#3], true, 10000, StorageLevel(true, true, false, true, 1), (Scan PhysicalRDD[cId#0L,name#1,age#2,gender#3]), None)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA variant of the explain method takes a Boolean argument and prints both logical and physical plans if the argument is true.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003epersist\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe persist method caches the source DataFrame in memory.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.persist\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSimilar to the persist method in the RDD class, a variant of the persist method in the DataFrame class allows you to specify the storage level for a persisted DataFrame.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eprintSchema\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe printSchema method prints the schema of the source DataFrame on the console in a tree format.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.printSchema()\n\nroot\n |-- cId: long (nullable \u003d false)\n |-- name: string (nullable \u003d true)\n |-- age: integer (nullable \u003d false)\n |-- gender: string (nullable \u003d true)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eregisterTempTable\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe registerTempTable method creates a temporary table in Hive metastore. It takes a table name as an argument.\u003c/p\u003e\n\u003cp\u003eA temporary table can be queried using the sql method in SQLContext or HiveContext. It is available only during the lifespan of the application that creates it.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.registerTempTable(\u0026quot;customer\u0026quot;)\nval countDF \u003d sqlContext.sql(\u0026quot;SELECT count(1) AS cnt FROM customer\u0026quot;)\n\ncountDF: org.apache.spark.sql.DataFrame \u003d [cnt: bigint]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that the sql method returns a DataFrame. Let’s discuss how to view or retrieve the contents of a DataFrame in a moment.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003etoDF\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe toDF method allows you to rename the columns in the source DataFrame. It takes the new names of the columns as arguments and returns a new DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval resultDF \u003d sqlContext.sql(\u0026quot;SELECT count(1) from customer\u0026quot;)\n\nresultDF: org.apache.spark.sql.DataFrame \u003d [_c0: bigint]\n\nval countDF \u003d resultDF.toDF(\u0026quot;cnt\u0026quot;)\n\ncountDF: org.apache.spark.sql.DataFrame \u003d [cnt: bigint]\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488326391902_1468710071",
      "id": "20170301-005951_335034470",
      "dateCreated": "Mar 1, 2017 12:59:51 AM",
      "dateStarted": "Mar 1, 2017 1:25:11 AM",
      "dateFinished": "Mar 1, 2017 1:25:11 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nLanguage-Integrated Query Methods\n---\n\n**agg**\n\nThe agg method performs specified aggregations on one or more columns in the source DataFrame and returns the result as a new DataFrame.\n~~~\nval aggregates \u003d productDF.agg(max(\"price\"), min(\"price\"), count(\"name\"))\n\naggregates: org.apache.spark.sql.DataFrame \u003d [max(price): double, min(price): double, count(name): bigint]\n\naggregates.show\n+----------+----------+-----------+\n|max(price)|min(price)|count(name)|\n+----------+----------+-----------+\n|    1200.0|     200.0|          6|\n+----------+----------+-----------+\n~~~\nLet’s discuss the show method in a little bit. Essentially, it displays the content of a DataFrame on the console.\n\n**apply**\n\nThe apply method takes the name of a column as an argument and returns the specified column in the source DataFrame as an instance of the Column class. The Column class provides operators for manipulating a column in a DataFrame.\n~~~\nval priceColumn \u003d productDF.apply(\"price\")\n\npriceColumn: org.apache.spark.sql.Column \u003d price\n\nval discountedPriceColumn \u003d priceColumn * 0.5\n\ndiscountedPriceColumn: org.apache.spark.sql.Column \u003d (price * 0.5)\n~~~~\n\nScala provides syntactic sugar that allows you to use productDF(\"price\") instead of productDF.apply(\"price\"). It automatically converts productDF(\"price\") to productDF.apply(\"price\"). So the preceding code can be rewritten, as shown next.\n\n~~~\nval priceColumn \u003d productDF(\"price\")\nval discountedPriceColumn \u003d priceColumn * 0.5\n~~~~\n\nAn instance of the Column class is generally used as an input to some of the DataFrame methods or functions defined in the Spark SQL library. Let’s revisit one of the examples discussed earlier.\n~~~\nval aggregates \u003d productDF.agg(max(\"price\"), min(\"price\"), count(\"name\"))\n~~~\nIt is a concise version of the statement shown next.\n~~~\nval aggregates \u003d productDF.agg(max(productDF(\"price\")), min(productDF(\"price\")),\n                               count(productDF(\"name\")))\n~~~\nThe expression productDF(\"price\") can also be written as $\"price\" for convenience. Thus, the following two expressions are equivalent.\n~~~\nval aggregates \u003d productDF.agg(max($\"price\"), min($\"price\"), count($\"name\"))\nval aggregates \u003d productDF.agg(max(productDF(\"price\")), min(productDF(\"price\")),\n                               count(productDF(\"name\")))\n~~~\nIf a method or function expects an instance of the Column class as an argument, you can use the $\"...\" notation to select a column in a DataFrame.\n\nIn summary, the following three statements are equivalent.\n~~~\nval aggregates \u003d productDF.agg(max(productDF(\"price\")), min(productDF(\"price\")),\n                               count(productDF(\"name\")))\nval aggregates \u003d productDF.agg(max(\"price\"), min(\"price\"), count(\"name\"))\nval aggregates \u003d productDF.agg(max($\"price\"), min($\"price\"), count($\"name\"))\n~~~\n**cube**\n\nThe cube method takes the names of one or more columns as arguments and returns a cube for multi-dimensional analysis. It is useful for generating cross-tabular reports.\n\nAssume you have a dataset that tracks sales along three dimensions: time, product and country. The cube method allows you to generate aggregates for all the possible combinations of the dimensions that you are interested in.\n~~~\ncase class SalesSummary(date: String, product: String, country: String, revenue: Double)\nval sales \u003d List(SalesSummary(\"01/01/2015\", \"iPhone\", \"USA\", 40000),\n                 SalesSummary(\"01/02/2015\", \"iPhone\", \"USA\", 30000),\n                 SalesSummary(\"01/01/2015\", \"iPhone\", \"China\", 10000),\n                 SalesSummary(\"01/02/2015\", \"iPhone\", \"China\", 5000),\n                 SalesSummary(\"01/01/2015\", \"S6\", \"USA\", 20000),\n                 SalesSummary(\"01/02/2015\", \"S6\", \"USA\", 10000),\n                 SalesSummary(\"01/01/2015\", \"S6\", \"China\", 9000),\n                 SalesSummary(\"01/02/2015\", \"S6\", \"China\", 6000))\n\nval salesDF \u003d sc.parallelize(sales).toDF()\n\nval salesCubeDF \u003d salesDF.cube($\"date\", $\"product\", $\"country\").sum(\"revenue\")\n\nsalesCubeDF: org.apache.spark.sql.DataFrame \u003d [date: string, product: string, country: string, sum(revenue): double]\n\nsalesCubeDF.withColumnRenamed(\"sum(revenue)\", \"total\").show(30)\n~~~\n+----------+-------+-------+--------+\n|      date|product|country|   total|\n+----------+-------+-------+--------+\n|01/01/2015|   null|    USA| 60000.0|\n|01/02/2015|     S6|   null| 16000.0|\n|01/01/2015| iPhone|   null| 50000.0|\n|01/01/2015|     S6|  China|  9000.0|\n|      null|   null|  China| 30000.0|\n|01/02/2015|     S6|    USA| 10000.0|\n|01/02/2015|   null|   null| 51000.0|\n|01/02/2015| iPhone|  China|  5000.0|\n|01/01/2015| iPhone|    USA| 40000.0|\n|01/01/2015|   null|  China| 19000.0|\n|01/02/2015|   null|    USA| 40000.0|\n|      null| iPhone|  China| 15000.0|\n|01/02/2015|     S6|  China|  6000.0|\n|01/01/2015| iPhone|  China| 10000.0|\n|01/02/2015|   null|  China| 11000.0|\n|      null| iPhone|   null| 85000.0|\n|      null| iPhone|    USA| 70000.0|\n|      null|     S6|   null| 45000.0|\n|      null|     S6|    USA| 30000.0|\n|01/01/2015|     S6|   null| 29000.0|\n|      null|   null|   null|130000.0|\n|01/02/2015| iPhone|   null| 35000.0|\n|01/01/2015|     S6|    USA| 20000.0|\n|      null|   null|    USA|100000.0|\n|01/01/2015|   null|   null| 79000.0|\n|      null|     S6|  China| 15000.0|\n|01/02/2015| iPhone|    USA| 30000.0|\n+----------+-------+-------+--------+\n\nIf you wanted to find the total sales of all products in the USA, you can use the following expression.\n~~~\nsalesCubeDF.filter(\"product IS null AND date IS null AND country\u003d\u0027USA\u0027\").show\n~~~\n+----+-------+-------+------------+\n|date|product|country|sum(revenue)|\n+----+-------+-------+------------+\n|null|   null|    USA|    100000.0|\n+----+-------+-------+------------+\n\nIf you wanted to know the subtotal of sales by product in the USA, you can use the following expression.\n~~~\nsalesCubeDF.filter(\"date IS null AND product IS NOT null AND country\u003d\u0027USA\u0027\").show\n~~~\n+----+-------+-------+------------+\n|date|product|country|sum(revenue)|\n+----+-------+-------+------------+\n|null| iPhone|    USA|     70000.0|\n|null|     S6|    USA|     30000.0|\n+----+-------+-------+------------+\n\n**distinct**\n\nThe distinct method returns a new DataFrame containing only the unique rows in the source DataFrame.\n~~~\nval dfWithoutDuplicates \u003d customerDF.distinct\n~~~\n**explode**\n\nThe explode method generates zero or more rows from a column using a user-provided function. It takes three arguments. The first argument is the input column, the second argument is the output column and the third argument is a user provided function that generates one or more values for the output column for each value in the input column.\n\nFor example, consider a dataset that has a text column containing contents of an email. Let’s assume that you want to split the email content into individual words and you want a row for each word in an email.\n~~~\ncase class Email(sender: String, recepient: String, subject: String, body: String)\nval emails \u003d List(Email(\"James\", \"Mary\", \"back\", \"just got back from vacation\"),\n                  Email(\"John\", \"Jessica\", \"money\", \"make million dollars\"),\n                  Email(\"Tim\", \"Kevin\", \"report\", \"send me sales report ASAP\"))\n\nval emailDF \u003d sc.parallelize(emails).toDF()\nval wordDF \u003d emailDF.explode(\"body\", \"word\") { body: String \u003d\u003e body.split(\" \")}\nwordDF.show\n\n+------+---------+-------+--------------------+--------+\n|sender|recepient|subject|                body|    word|\n+------+---------+-------+--------------------+--------+\n| James|     Mary|   back|just got back fro...|    just|\n| James|     Mary|   back|just got back fro...|     got|\n| James|     Mary|   back|just got back fro...|    back|\n| James|     Mary|   back|just got back fro...|    from|\n| James|     Mary|   back|just got back fro...|vacation|\n|  John|  Jessica|  money|make million dollars|    make|\n|  John|  Jessica|  money|make million dollars| million|\n|  John|  Jessica|  money|make million dollars| dollars|\n|   Tim|    Kevin| report|send me sales rep...|    send|\n|   Tim|    Kevin| report|send me sales rep...|      me|\n|   Tim|    Kevin| report|send me sales rep...|   sales|\n|   Tim|    Kevin| report|send me sales rep...|  report|\n|   Tim|    Kevin| report|send me sales rep...|    ASAP|\n+------+---------+-------+--------------------+--------+\n~~~\n**filter**\n\nThe filter method filters rows in the source DataFrame using a SQL expression provided to it as an argument. It returns a new DataFrame containing only the filtered rows. The SQL expression can be passed as a string argument.\n~~~\nval filteredDF \u003d customerDF.filter(\"age \u003e 25\")\n\nfilteredDF: org.apache.spark.sql.DataFrame \u003d [cId: bigint, name: string, age: int, gender: string]\n\nfilteredDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  3|    John| 31|     M|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n~~~\n\nA variant of the filter method allows a filter condition to be specified using the Column type.\n~~~\nval filteredDF \u003d customerDF.filter($\"age\" \u003e 25)\n~~~\nAs mentioned earlier, the preceding code is a short-hand for the following code.\n~~~\nval filteredDF \u003d customerDF.filter(customerDF(\"age\") \u003e 25)\n~~~\n**groupBy**\n\nThe groupBy method groups the rows in the source DataFrame using the columns provided to it as arguments. Aggregation can be performed on the grouped data returned by this method.\n~~~\nval countByGender \u003d customerDF.groupBy(\"gender\").count\n\ncountByGender: org.apache.spark.sql.DataFrame \u003d [gender: string, count: bigint]\n\ncountByGender.show\n\n+------+-----+\n|gender|count|\n+------+-----+\n|     F|    3|\n|     M|    3|\n+------+-----+\n~~~\n~~~\nval revenueByProductDF \u003d salesDF.groupBy(\"product\").sum(\"revenue\")\n\nrevenueByProductDF: org.apache.spark.sql.DataFrame \u003d [product: string, sum(revenue): double]\n\nrevenueByProductDF.show\n\n+-------+------------+\n|product|sum(revenue)|\n+-------+------------+\n| iPhone|     85000.0|\n|     S6|     45000.0|\n+-------+------------+\n~~~\n**intersect**\n\nThe intersect method takes a DataFrame as an argument and returns a new DataFrame containing only the rows in both the input and source DataFrame.\n~~~\nval customers2 \u003d List(Customer(11, \"Jackson\", 21, \"M\"),\n                     Customer(12, \"Emma\", 25, \"F\"),\n                     Customer(13, \"Olivia\", 31, \"F\"),\n                     Customer(4, \"Jennifer\", 45, \"F\"),\n                     Customer(5, \"Robert\", 41, \"M\"),\n                     Customer(6, \"Sandra\", 45, \"F\"))\n\nval customer2DF \u003d sc.parallelize(customers2).toDF()\nval commonCustomersDF \u003d customerDF.intersect(customer2DF)\ncommonCustomersDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  6|  Sandra| 45|     F|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n+---+--------+---+------+\n~~~\n**join**\n\nThe join method performs a SQL join of the source DataFrame with another DataFrame. It takes three arguments, a DataFrame, a join expression and a join type.\n~~~\ncase class Transaction(tId: Long, custId: Long, prodId: Long, date: String, city: String)\nval transactions \u003d List(Transaction(1, 5, 3, \"01/01/2015\", \"San Francisco\"),\n                        Transaction(2, 6, 1, \"01/02/2015\", \"San Jose\"),\n                        Transaction(3, 1, 6, \"01/01/2015\", \"Boston\"),\n                        Transaction(4, 200, 400, \"01/02/2015\", \"Palo Alto\"),\n                        Transaction(6, 100, 100, \"01/02/2015\", \"Mountain View\"))\n\nval transactionDF \u003d sc.parallelize(transactions).toDF()\nval innerDF \u003d transactionDF.join(customerDF, $\"custId\" \u003d\u003d\u003d $\"cId\", \"inner\")\n\ninnerDF.show\n\n+---+------+------+----------+-------------+---+------+---+------+\n|tId|custId|prodId|      date|         city|cId|  name|age|gender|\n+---+------+------+----------+-------------+---+------+---+------+\n|  1|     5|     3|01/01/2015|San Francisco|  5|Robert| 41|     M|\n|  2|     6|     1|01/02/2015|     San Jose|  6|Sandra| 45|     F|\n|  3|     1|     6|01/01/2015|       Boston|  1| James| 21|     M|\n+---+------+------+----------+-------------+---+------+---+------+\n\nval outerDF \u003d transactionDF.join(customerDF, $\"custId\" \u003d\u003d\u003d $\"cId\", \"outer\")\nouterDF.show\n\n+----+------+------+----------+-------------+----+--------+----+------+\n| tId|custId|prodId|      date|         city| cId|    name| age|gender|\n+----+------+------+----------+-------------+----+--------+----+------+\n|   6|   100|   100|01/02/2015|Mountain View|null|    null|null|  null|\n|   4|   200|   400|01/02/2015|    Palo Alto|null|    null|null|  null|\n|   3|     1|     6|01/01/2015|       Boston|   1|   James|  21|     M|\n|null|  null|  null|      null|         null|   2|     Liz|  25|     F|\n|null|  null|  null|      null|         null|   3|    John|  31|     M|\n|null|  null|  null|      null|         null|   4|Jennifer|  45|     F|\n|   1|     5|     3|01/01/2015|San Francisco|   5|  Robert|  41|     M|\n|   2|     6|     1|01/02/2015|     San Jose|   6|  Sandra|  45|     F|\n+----+------+------+----------+-------------+----+--------+----+------+\n\nval leftOuterDF \u003d transactionDF.join(customerDF, $\"custId\" \u003d\u003d\u003d $\"cId\", \"left_outer\")\nleftOuterDF.show\n\n+---+------+------+----------+-------------+----+------+----+------+\n|tId|custId|prodId|      date|         city| cId|  name| age|gender|\n+---+------+------+----------+-------------+----+------+----+------+\n|  1|     5|     3|01/01/2015|San Francisco|   5|Robert|  41|     M|\n|  2|     6|     1|01/02/2015|     San Jose|   6|Sandra|  45|     F|\n|  3|     1|     6|01/01/2015|       Boston|   1| James|  21|     M|\n|  4|   200|   400|01/02/2015|    Palo Alto|null|  null|null|  null|\n|  6|   100|   100|01/02/2015|Mountain View|null|  null|null|  null|\n+---+------+------+----------+-------------+----+------+----+------+\n\nval rightOuterDF \u003d transactionDF.join(customerDF, $\"custId\" \u003d\u003d\u003d $\"cId\", \"right_outer\")\nrightOuterDF.show\n\n+----+------+------+----------+-------------+---+--------+---+------+\n| tId|custId|prodId|      date|         city|cId|    name|age|gender|\n+----+------+------+----------+-------------+---+--------+---+------+\n|   3|     1|     6|01/01/2015|       Boston|  1|   James| 21|     M|\n|null|  null|  null|      null|         null|  2|     Liz| 25|     F|\n|null|  null|  null|      null|         null|  3|    John| 31|     M|\n|null|  null|  null|      null|         null|  4|Jennifer| 45|     F|\n|   1|     5|     3|01/01/2015|San Francisco|  5|  Robert| 41|     M|\n|   2|     6|     1|01/02/2015|     San Jose|  6|  Sandra| 45|     F|\n+----+------+------+----------+-------------+---+--------+---+------+\n~~~\n**limit**\n\nThe limit method returns a DataFrame containing the specified number of rows from the source DataFrame.\n~~~\nval fiveCustomerDF \u003d customerDF.limit(5)\nfiveCustomer.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  1|   James| 21|     M|\n|  2|     Liz| 25|     F|\n|  3|    John| 31|     M|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n+---+--------+---+------+\n~~~\n**orderBy**\n\nThe orderBy method returns a DataFrame sorted by the given columns. It takes the names of one or more columns as arguments.\n~~~\nval sortedDF \u003d customerDF.orderBy(\"name\")\nsortedDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  1|   James| 21|     M|\n|  4|Jennifer| 45|     F|\n|  3|    John| 31|     M|\n|  2|     Liz| 25|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n~~~\nBy default, the orderBy method sorts in ascending order. You can explicitly specify the sorting order using a Column expression, as shown next.\n~~~\nval sortedByAgeNameDF \u003d customerDF.sort($\"age\".desc, $\"name\".asc)\nsortedByAgeNameDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  4|Jennifer| 45|     F|\n|  6|  Sandra| 45|     F|\n|  5|  Robert| 41|     M|\n|  3|    John| 31|     M|\n|  2|     Liz| 25|     F|\n|  1|   James| 21|     M|\n+---+--------+---+------+\n~~~\n**randomSplit**\n\nThe randomSplit method splits the source DataFrame into multiple DataFrames. It takes an array of weights as argument and returns an array of DataFrames. It is a useful method for machine learning, where you want to split the raw dataset into training, validation and test datasets.\n~~~\nval dfArray \u003d homeDF.randomSplit(Array(0.6, 0.2, 0.2))\ndfArray(0).count\ndfArray(1).count\ndfArray(2).count\n~~~\n**rollup**\n\nThe rollup method takes the names of one or more columns as arguments and returns a multi-dimensional rollup. It is useful for subaggregation along a hierarchical dimension such as geography or time.\n\nAssume you have a dataset that tracks annual sales by city, state and country. The rollup method can be used to calculate both grand total and subtotals by city, state, and country.\n~~~\ncase class SalesByCity(year: Int, city: String, state: String,\n                       country: String, revenue: Double)\nval salesByCity \u003d List(SalesByCity(2014, \"Boston\", \"MA\", \"USA\", 2000),\n                       SalesByCity(2015, \"Boston\", \"MA\", \"USA\", 3000),\n                       SalesByCity(2014, \"Cambridge\", \"MA\", \"USA\", 2000),\n                       SalesByCity(2015, \"Cambridge\", \"MA\", \"USA\", 3000),\n                       SalesByCity(2014, \"Palo Alto\", \"CA\", \"USA\", 4000),\n                       SalesByCity(2015, \"Palo Alto\", \"CA\", \"USA\", 6000),\n                       SalesByCity(2014, \"Pune\", \"MH\", \"India\", 1000),\n                       SalesByCity(2015, \"Pune\", \"MH\", \"India\", 1000),\n                       SalesByCity(2015, \"Mumbai\", \"MH\", \"India\", 1000),\n                       SalesByCity(2014, \"Mumbai\", \"MH\", \"India\", 2000))\n\nval salesByCityDF \u003d sc.parallelize(salesByCity).toDF()\nval rollup \u003d salesByCityDF.rollup($\"country\", $\"state\", $\"city\").sum(\"revenue\")\nrollup.show\n\n+-------+-----+---------+------------+\n|country|state|     city|sum(revenue)|\n+-------+-----+---------+------------+\n|  India|   MH|   Mumbai|      3000.0|\n|    USA|   MA|Cambridge|      5000.0|\n|  India|   MH|     Pune|      2000.0|\n|    USA|   MA|   Boston|      5000.0|\n|    USA|   MA|     null|     10000.0|\n|    USA| null|     null|     20000.0|\n|    USA|   CA|     null|     10000.0|\n|   null| null|     null|     25000.0|\n|  India|   MH|     null|      5000.0|\n|    USA|   CA|Palo Alto|     10000.0|\n|  India| null|     null|      5000.0|\n+-------+-----+---------+------------+\n~~~\n**sample**\n\nThe sample method returns a DataFrame containing the specified fraction of the rows in the source DataFrame. It takes two arguments. The first argument is a Boolean value indicating whether sampling should be done with replacement. The second argument specifies the fraction of the rows that should be returned.\n~~~\nval sampleDF \u003d homeDF.sample(true, 0.10)\n~~~\n**select**\n\nThe select method returns a DataFrame containing only the specified columns from the source DataFrame.\n~~~\nval namesAgeDF \u003d customerDF.select(\"name\", \"age\")\nnamesAgeDF.show\n\n+--------+---+\n|    name|age|\n+--------+---+\n|   James| 21|\n|     Liz| 25|\n|    John| 31|\n|Jennifer| 45|\n|  Robert| 41|\n|  Sandra| 45|\n+--------+---+\n~~~\nA variant of the select method allows one or more Column expressions as arguments.\n~~~\nval newAgeDF \u003d customerDF.select($\"name\", $\"age\" + 10)\nnewAgeDF.show\n\n+--------+----------+\n|    name|(age + 10)|\n+--------+----------+\n|   James|        31|\n|     Liz|        35|\n|    John|        41|\n|Jennifer|        55|\n|  Robert|        51|\n|  Sandra|        55|\n+--------+----------+\n~~~\n**selectExpr**\n\nThe selectExpr method accepts one or more SQL expressions as arguments and returns a DataFrame generated by executing the specified SQL expressions.\n~~~\nval newCustomerDF \u003d customerDF.selectExpr(\"name\", \"age + 10  AS new_age\",\n                                          \"IF(gender \u003d \u0027M\u0027, true, false) AS male\")\n\nnewCustomerDF.show\n\n+--------+-------+-----+\n|    name|new_age| male|\n+--------+-------+-----+\n|   James|     31| true|\n|     Liz|     35|false|\n|    John|     41| true|\n|Jennifer|     55|false|\n|  Robert|     51| true|\n|  Sandra|     55|false|\n+--------+-------+-----+\n~~~\n**withColumn**\n\nThe withColumn method adds a new column to or replaces an existing column in the source DataFrame and returns a new DataFrame. It takes two arguments. The first argument is the name of the new column and the second argument is an expression for generating the values of the new column.\n~~~\nval newProductDF \u003d productDF.withColumn(\"profit\", $\"price\" - $\"cost\")\nnewProductDF.show\n\n+---+-------+------+-----+------+\n|pId|   name| price| cost|profit|\n+---+-------+------+-----+------+\n|  1| iPhone| 600.0|400.0| 200.0|\n|  2| Galaxy| 500.0|400.0| 100.0|\n|  3|   iPad| 400.0|300.0| 100.0|\n|  4| Kindle| 200.0|100.0| 100.0|\n|  5|MacBook|1200.0|900.0| 300.0|\n|  6|   Dell| 500.0|400.0| 100.0|\n+---+-------+------+-----+------+\n~~~\n\n\n\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 12:33:06 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eLanguage-Integrated Query Methods\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eagg\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe agg method performs specified aggregations on one or more columns in the source DataFrame and returns the result as a new DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval aggregates \u003d productDF.agg(max(\u0026quot;price\u0026quot;), min(\u0026quot;price\u0026quot;), count(\u0026quot;name\u0026quot;))\n\naggregates: org.apache.spark.sql.DataFrame \u003d [max(price): double, min(price): double, count(name): bigint]\n\naggregates.show\n+----------+----------+-----------+\n|max(price)|min(price)|count(name)|\n+----------+----------+-----------+\n|    1200.0|     200.0|          6|\n+----------+----------+-----------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet’s discuss the show method in a little bit. Essentially, it displays the content of a DataFrame on the console.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eapply\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe apply method takes the name of a column as an argument and returns the specified column in the source DataFrame as an instance of the Column class. The Column class provides operators for manipulating a column in a DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval priceColumn \u003d productDF.apply(\u0026quot;price\u0026quot;)\n\npriceColumn: org.apache.spark.sql.Column \u003d price\n\nval discountedPriceColumn \u003d priceColumn * 0.5\n\ndiscountedPriceColumn: org.apache.spark.sql.Column \u003d (price * 0.5)\n~~~~\n\nScala provides syntactic sugar that allows you to use productDF(\u0026quot;price\u0026quot;) instead of productDF.apply(\u0026quot;price\u0026quot;). It automatically converts productDF(\u0026quot;price\u0026quot;) to productDF.apply(\u0026quot;price\u0026quot;). So the preceding code can be rewritten, as shown next.\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eval priceColumn \u003d productDF(\u0026ldquo;price\u0026rdquo;)\u003cbr/\u003eval discountedPriceColumn \u003d priceColumn * 0.5\u003cbr/\u003e~~~~\u003c/p\u003e\n\u003cp\u003eAn instance of the Column class is generally used as an input to some of the DataFrame methods or functions defined in the Spark SQL library. Let’s revisit one of the examples discussed earlier.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval aggregates \u003d productDF.agg(max(\u0026quot;price\u0026quot;), min(\u0026quot;price\u0026quot;), count(\u0026quot;name\u0026quot;))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIt is a concise version of the statement shown next.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval aggregates \u003d productDF.agg(max(productDF(\u0026quot;price\u0026quot;)), min(productDF(\u0026quot;price\u0026quot;)),\n                               count(productDF(\u0026quot;name\u0026quot;)))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe expression productDF(\u0026ldquo;price\u0026rdquo;) can also be written as $\u0026ldquo;price\u0026rdquo; for convenience. Thus, the following two expressions are equivalent.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval aggregates \u003d productDF.agg(max($\u0026quot;price\u0026quot;), min($\u0026quot;price\u0026quot;), count($\u0026quot;name\u0026quot;))\nval aggregates \u003d productDF.agg(max(productDF(\u0026quot;price\u0026quot;)), min(productDF(\u0026quot;price\u0026quot;)),\n                               count(productDF(\u0026quot;name\u0026quot;)))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf a method or function expects an instance of the Column class as an argument, you can use the $\u0026ldquo;\u0026hellip;\u0026rdquo; notation to select a column in a DataFrame.\u003c/p\u003e\n\u003cp\u003eIn summary, the following three statements are equivalent.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval aggregates \u003d productDF.agg(max(productDF(\u0026quot;price\u0026quot;)), min(productDF(\u0026quot;price\u0026quot;)),\n                               count(productDF(\u0026quot;name\u0026quot;)))\nval aggregates \u003d productDF.agg(max(\u0026quot;price\u0026quot;), min(\u0026quot;price\u0026quot;), count(\u0026quot;name\u0026quot;))\nval aggregates \u003d productDF.agg(max($\u0026quot;price\u0026quot;), min($\u0026quot;price\u0026quot;), count($\u0026quot;name\u0026quot;))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003ecube\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe cube method takes the names of one or more columns as arguments and returns a cube for multi-dimensional analysis. It is useful for generating cross-tabular reports.\u003c/p\u003e\n\u003cp\u003eAssume you have a dataset that tracks sales along three dimensions: time, product and country. The cube method allows you to generate aggregates for all the possible combinations of the dimensions that you are interested in.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecase class SalesSummary(date: String, product: String, country: String, revenue: Double)\nval sales \u003d List(SalesSummary(\u0026quot;01/01/2015\u0026quot;, \u0026quot;iPhone\u0026quot;, \u0026quot;USA\u0026quot;, 40000),\n                 SalesSummary(\u0026quot;01/02/2015\u0026quot;, \u0026quot;iPhone\u0026quot;, \u0026quot;USA\u0026quot;, 30000),\n                 SalesSummary(\u0026quot;01/01/2015\u0026quot;, \u0026quot;iPhone\u0026quot;, \u0026quot;China\u0026quot;, 10000),\n                 SalesSummary(\u0026quot;01/02/2015\u0026quot;, \u0026quot;iPhone\u0026quot;, \u0026quot;China\u0026quot;, 5000),\n                 SalesSummary(\u0026quot;01/01/2015\u0026quot;, \u0026quot;S6\u0026quot;, \u0026quot;USA\u0026quot;, 20000),\n                 SalesSummary(\u0026quot;01/02/2015\u0026quot;, \u0026quot;S6\u0026quot;, \u0026quot;USA\u0026quot;, 10000),\n                 SalesSummary(\u0026quot;01/01/2015\u0026quot;, \u0026quot;S6\u0026quot;, \u0026quot;China\u0026quot;, 9000),\n                 SalesSummary(\u0026quot;01/02/2015\u0026quot;, \u0026quot;S6\u0026quot;, \u0026quot;China\u0026quot;, 6000))\n\nval salesDF \u003d sc.parallelize(sales).toDF()\n\nval salesCubeDF \u003d salesDF.cube($\u0026quot;date\u0026quot;, $\u0026quot;product\u0026quot;, $\u0026quot;country\u0026quot;).sum(\u0026quot;revenue\u0026quot;)\n\nsalesCubeDF: org.apache.spark.sql.DataFrame \u003d [date: string, product: string, country: string, sum(revenue): double]\n\nsalesCubeDF.withColumnRenamed(\u0026quot;sum(revenue)\u0026quot;, \u0026quot;total\u0026quot;).show(30)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e+\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026ndash;+\u003cbr/\u003e| date|product|country| total|\u003cbr/\u003e+\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026ndash;+\u003cbr/\u003e|01/01/2015| null| USA| 60000.0|\u003cbr/\u003e|01/02/2015| S6| null| 16000.0|\u003cbr/\u003e|01/01/2015| iPhone| null| 50000.0|\u003cbr/\u003e|01/01/2015| S6| China| 9000.0|\u003cbr/\u003e| null| null| China| 30000.0|\u003cbr/\u003e|01/02/2015| S6| USA| 10000.0|\u003cbr/\u003e|01/02/2015| null| null| 51000.0|\u003cbr/\u003e|01/02/2015| iPhone| China| 5000.0|\u003cbr/\u003e|01/01/2015| iPhone| USA| 40000.0|\u003cbr/\u003e|01/01/2015| null| China| 19000.0|\u003cbr/\u003e|01/02/2015| null| USA| 40000.0|\u003cbr/\u003e| null| iPhone| China| 15000.0|\u003cbr/\u003e|01/02/2015| S6| China| 6000.0|\u003cbr/\u003e|01/01/2015| iPhone| China| 10000.0|\u003cbr/\u003e|01/02/2015| null| China| 11000.0|\u003cbr/\u003e| null| iPhone| null| 85000.0|\u003cbr/\u003e| null| iPhone| USA| 70000.0|\u003cbr/\u003e| null| S6| null| 45000.0|\u003cbr/\u003e| null| S6| USA| 30000.0|\u003cbr/\u003e|01/01/2015| S6| null| 29000.0|\u003cbr/\u003e| null| null| null|130000.0|\u003cbr/\u003e|01/02/2015| iPhone| null| 35000.0|\u003cbr/\u003e|01/01/2015| S6| USA| 20000.0|\u003cbr/\u003e| null| null| USA|100000.0|\u003cbr/\u003e|01/01/2015| null| null| 79000.0|\u003cbr/\u003e| null| S6| China| 15000.0|\u003cbr/\u003e|01/02/2015| iPhone| USA| 30000.0|\u003cbr/\u003e+\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026ndash;+\u003c/p\u003e\n\u003cp\u003eIf you wanted to find the total sales of all products in the USA, you can use the following expression.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esalesCubeDF.filter(\u0026quot;product IS null AND date IS null AND country\u003d\u0026#39;USA\u0026#39;\u0026quot;).show\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e+\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u003cbr/\u003e|date|product|country|sum(revenue)|\u003cbr/\u003e+\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u003cbr/\u003e|null| null| USA| 100000.0|\u003cbr/\u003e+\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u003c/p\u003e\n\u003cp\u003eIf you wanted to know the subtotal of sales by product in the USA, you can use the following expression.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esalesCubeDF.filter(\u0026quot;date IS null AND product IS NOT null AND country\u003d\u0026#39;USA\u0026#39;\u0026quot;).show\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e+\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u003cbr/\u003e|date|product|country|sum(revenue)|\u003cbr/\u003e+\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u003cbr/\u003e|null| iPhone| USA| 70000.0|\u003cbr/\u003e|null| S6| USA| 30000.0|\u003cbr/\u003e+\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003edistinct\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe distinct method returns a new DataFrame containing only the unique rows in the source DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval dfWithoutDuplicates \u003d customerDF.distinct\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eexplode\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe explode method generates zero or more rows from a column using a user-provided function. It takes three arguments. The first argument is the input column, the second argument is the output column and the third argument is a user provided function that generates one or more values for the output column for each value in the input column.\u003c/p\u003e\n\u003cp\u003eFor example, consider a dataset that has a text column containing contents of an email. Let’s assume that you want to split the email content into individual words and you want a row for each word in an email.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecase class Email(sender: String, recepient: String, subject: String, body: String)\nval emails \u003d List(Email(\u0026quot;James\u0026quot;, \u0026quot;Mary\u0026quot;, \u0026quot;back\u0026quot;, \u0026quot;just got back from vacation\u0026quot;),\n                  Email(\u0026quot;John\u0026quot;, \u0026quot;Jessica\u0026quot;, \u0026quot;money\u0026quot;, \u0026quot;make million dollars\u0026quot;),\n                  Email(\u0026quot;Tim\u0026quot;, \u0026quot;Kevin\u0026quot;, \u0026quot;report\u0026quot;, \u0026quot;send me sales report ASAP\u0026quot;))\n\nval emailDF \u003d sc.parallelize(emails).toDF()\nval wordDF \u003d emailDF.explode(\u0026quot;body\u0026quot;, \u0026quot;word\u0026quot;) { body: String \u003d\u0026gt; body.split(\u0026quot; \u0026quot;)}\nwordDF.show\n\n+------+---------+-------+--------------------+--------+\n|sender|recepient|subject|                body|    word|\n+------+---------+-------+--------------------+--------+\n| James|     Mary|   back|just got back fro...|    just|\n| James|     Mary|   back|just got back fro...|     got|\n| James|     Mary|   back|just got back fro...|    back|\n| James|     Mary|   back|just got back fro...|    from|\n| James|     Mary|   back|just got back fro...|vacation|\n|  John|  Jessica|  money|make million dollars|    make|\n|  John|  Jessica|  money|make million dollars| million|\n|  John|  Jessica|  money|make million dollars| dollars|\n|   Tim|    Kevin| report|send me sales rep...|    send|\n|   Tim|    Kevin| report|send me sales rep...|      me|\n|   Tim|    Kevin| report|send me sales rep...|   sales|\n|   Tim|    Kevin| report|send me sales rep...|  report|\n|   Tim|    Kevin| report|send me sales rep...|    ASAP|\n+------+---------+-------+--------------------+--------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003efilter\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe filter method filters rows in the source DataFrame using a SQL expression provided to it as an argument. It returns a new DataFrame containing only the filtered rows. The SQL expression can be passed as a string argument.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval filteredDF \u003d customerDF.filter(\u0026quot;age \u0026gt; 25\u0026quot;)\n\nfilteredDF: org.apache.spark.sql.DataFrame \u003d [cId: bigint, name: string, age: int, gender: string]\n\nfilteredDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  3|    John| 31|     M|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA variant of the filter method allows a filter condition to be specified using the Column type.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval filteredDF \u003d customerDF.filter($\u0026quot;age\u0026quot; \u0026gt; 25)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAs mentioned earlier, the preceding code is a short-hand for the following code.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval filteredDF \u003d customerDF.filter(customerDF(\u0026quot;age\u0026quot;) \u0026gt; 25)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003egroupBy\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe groupBy method groups the rows in the source DataFrame using the columns provided to it as arguments. Aggregation can be performed on the grouped data returned by this method.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval countByGender \u003d customerDF.groupBy(\u0026quot;gender\u0026quot;).count\n\ncountByGender: org.apache.spark.sql.DataFrame \u003d [gender: string, count: bigint]\n\ncountByGender.show\n\n+------+-----+\n|gender|count|\n+------+-----+\n|     F|    3|\n|     M|    3|\n+------+-----+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eval revenueByProductDF \u003d salesDF.groupBy(\u0026quot;product\u0026quot;).sum(\u0026quot;revenue\u0026quot;)\n\nrevenueByProductDF: org.apache.spark.sql.DataFrame \u003d [product: string, sum(revenue): double]\n\nrevenueByProductDF.show\n\n+-------+------------+\n|product|sum(revenue)|\n+-------+------------+\n| iPhone|     85000.0|\n|     S6|     45000.0|\n+-------+------------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eintersect\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe intersect method takes a DataFrame as an argument and returns a new DataFrame containing only the rows in both the input and source DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval customers2 \u003d List(Customer(11, \u0026quot;Jackson\u0026quot;, 21, \u0026quot;M\u0026quot;),\n                     Customer(12, \u0026quot;Emma\u0026quot;, 25, \u0026quot;F\u0026quot;),\n                     Customer(13, \u0026quot;Olivia\u0026quot;, 31, \u0026quot;F\u0026quot;),\n                     Customer(4, \u0026quot;Jennifer\u0026quot;, 45, \u0026quot;F\u0026quot;),\n                     Customer(5, \u0026quot;Robert\u0026quot;, 41, \u0026quot;M\u0026quot;),\n                     Customer(6, \u0026quot;Sandra\u0026quot;, 45, \u0026quot;F\u0026quot;))\n\nval customer2DF \u003d sc.parallelize(customers2).toDF()\nval commonCustomersDF \u003d customerDF.intersect(customer2DF)\ncommonCustomersDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  6|  Sandra| 45|     F|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n+---+--------+---+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003ejoin\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe join method performs a SQL join of the source DataFrame with another DataFrame. It takes three arguments, a DataFrame, a join expression and a join type.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecase class Transaction(tId: Long, custId: Long, prodId: Long, date: String, city: String)\nval transactions \u003d List(Transaction(1, 5, 3, \u0026quot;01/01/2015\u0026quot;, \u0026quot;San Francisco\u0026quot;),\n                        Transaction(2, 6, 1, \u0026quot;01/02/2015\u0026quot;, \u0026quot;San Jose\u0026quot;),\n                        Transaction(3, 1, 6, \u0026quot;01/01/2015\u0026quot;, \u0026quot;Boston\u0026quot;),\n                        Transaction(4, 200, 400, \u0026quot;01/02/2015\u0026quot;, \u0026quot;Palo Alto\u0026quot;),\n                        Transaction(6, 100, 100, \u0026quot;01/02/2015\u0026quot;, \u0026quot;Mountain View\u0026quot;))\n\nval transactionDF \u003d sc.parallelize(transactions).toDF()\nval innerDF \u003d transactionDF.join(customerDF, $\u0026quot;custId\u0026quot; \u003d\u003d\u003d $\u0026quot;cId\u0026quot;, \u0026quot;inner\u0026quot;)\n\ninnerDF.show\n\n+---+------+------+----------+-------------+---+------+---+------+\n|tId|custId|prodId|      date|         city|cId|  name|age|gender|\n+---+------+------+----------+-------------+---+------+---+------+\n|  1|     5|     3|01/01/2015|San Francisco|  5|Robert| 41|     M|\n|  2|     6|     1|01/02/2015|     San Jose|  6|Sandra| 45|     F|\n|  3|     1|     6|01/01/2015|       Boston|  1| James| 21|     M|\n+---+------+------+----------+-------------+---+------+---+------+\n\nval outerDF \u003d transactionDF.join(customerDF, $\u0026quot;custId\u0026quot; \u003d\u003d\u003d $\u0026quot;cId\u0026quot;, \u0026quot;outer\u0026quot;)\nouterDF.show\n\n+----+------+------+----------+-------------+----+--------+----+------+\n| tId|custId|prodId|      date|         city| cId|    name| age|gender|\n+----+------+------+----------+-------------+----+--------+----+------+\n|   6|   100|   100|01/02/2015|Mountain View|null|    null|null|  null|\n|   4|   200|   400|01/02/2015|    Palo Alto|null|    null|null|  null|\n|   3|     1|     6|01/01/2015|       Boston|   1|   James|  21|     M|\n|null|  null|  null|      null|         null|   2|     Liz|  25|     F|\n|null|  null|  null|      null|         null|   3|    John|  31|     M|\n|null|  null|  null|      null|         null|   4|Jennifer|  45|     F|\n|   1|     5|     3|01/01/2015|San Francisco|   5|  Robert|  41|     M|\n|   2|     6|     1|01/02/2015|     San Jose|   6|  Sandra|  45|     F|\n+----+------+------+----------+-------------+----+--------+----+------+\n\nval leftOuterDF \u003d transactionDF.join(customerDF, $\u0026quot;custId\u0026quot; \u003d\u003d\u003d $\u0026quot;cId\u0026quot;, \u0026quot;left_outer\u0026quot;)\nleftOuterDF.show\n\n+---+------+------+----------+-------------+----+------+----+------+\n|tId|custId|prodId|      date|         city| cId|  name| age|gender|\n+---+------+------+----------+-------------+----+------+----+------+\n|  1|     5|     3|01/01/2015|San Francisco|   5|Robert|  41|     M|\n|  2|     6|     1|01/02/2015|     San Jose|   6|Sandra|  45|     F|\n|  3|     1|     6|01/01/2015|       Boston|   1| James|  21|     M|\n|  4|   200|   400|01/02/2015|    Palo Alto|null|  null|null|  null|\n|  6|   100|   100|01/02/2015|Mountain View|null|  null|null|  null|\n+---+------+------+----------+-------------+----+------+----+------+\n\nval rightOuterDF \u003d transactionDF.join(customerDF, $\u0026quot;custId\u0026quot; \u003d\u003d\u003d $\u0026quot;cId\u0026quot;, \u0026quot;right_outer\u0026quot;)\nrightOuterDF.show\n\n+----+------+------+----------+-------------+---+--------+---+------+\n| tId|custId|prodId|      date|         city|cId|    name|age|gender|\n+----+------+------+----------+-------------+---+--------+---+------+\n|   3|     1|     6|01/01/2015|       Boston|  1|   James| 21|     M|\n|null|  null|  null|      null|         null|  2|     Liz| 25|     F|\n|null|  null|  null|      null|         null|  3|    John| 31|     M|\n|null|  null|  null|      null|         null|  4|Jennifer| 45|     F|\n|   1|     5|     3|01/01/2015|San Francisco|  5|  Robert| 41|     M|\n|   2|     6|     1|01/02/2015|     San Jose|  6|  Sandra| 45|     F|\n+----+------+------+----------+-------------+---+--------+---+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003elimit\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe limit method returns a DataFrame containing the specified number of rows from the source DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval fiveCustomerDF \u003d customerDF.limit(5)\nfiveCustomer.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  1|   James| 21|     M|\n|  2|     Liz| 25|     F|\n|  3|    John| 31|     M|\n|  4|Jennifer| 45|     F|\n|  5|  Robert| 41|     M|\n+---+--------+---+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eorderBy\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe orderBy method returns a DataFrame sorted by the given columns. It takes the names of one or more columns as arguments.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval sortedDF \u003d customerDF.orderBy(\u0026quot;name\u0026quot;)\nsortedDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  1|   James| 21|     M|\n|  4|Jennifer| 45|     F|\n|  3|    John| 31|     M|\n|  2|     Liz| 25|     F|\n|  5|  Robert| 41|     M|\n|  6|  Sandra| 45|     F|\n+---+--------+---+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBy default, the orderBy method sorts in ascending order. You can explicitly specify the sorting order using a Column expression, as shown next.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval sortedByAgeNameDF \u003d customerDF.sort($\u0026quot;age\u0026quot;.desc, $\u0026quot;name\u0026quot;.asc)\nsortedByAgeNameDF.show\n\n+---+--------+---+------+\n|cId|    name|age|gender|\n+---+--------+---+------+\n|  4|Jennifer| 45|     F|\n|  6|  Sandra| 45|     F|\n|  5|  Robert| 41|     M|\n|  3|    John| 31|     M|\n|  2|     Liz| 25|     F|\n|  1|   James| 21|     M|\n+---+--------+---+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003erandomSplit\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe randomSplit method splits the source DataFrame into multiple DataFrames. It takes an array of weights as argument and returns an array of DataFrames. It is a useful method for machine learning, where you want to split the raw dataset into training, validation and test datasets.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval dfArray \u003d homeDF.randomSplit(Array(0.6, 0.2, 0.2))\ndfArray(0).count\ndfArray(1).count\ndfArray(2).count\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003erollup\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe rollup method takes the names of one or more columns as arguments and returns a multi-dimensional rollup. It is useful for subaggregation along a hierarchical dimension such as geography or time.\u003c/p\u003e\n\u003cp\u003eAssume you have a dataset that tracks annual sales by city, state and country. The rollup method can be used to calculate both grand total and subtotals by city, state, and country.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecase class SalesByCity(year: Int, city: String, state: String,\n                       country: String, revenue: Double)\nval salesByCity \u003d List(SalesByCity(2014, \u0026quot;Boston\u0026quot;, \u0026quot;MA\u0026quot;, \u0026quot;USA\u0026quot;, 2000),\n                       SalesByCity(2015, \u0026quot;Boston\u0026quot;, \u0026quot;MA\u0026quot;, \u0026quot;USA\u0026quot;, 3000),\n                       SalesByCity(2014, \u0026quot;Cambridge\u0026quot;, \u0026quot;MA\u0026quot;, \u0026quot;USA\u0026quot;, 2000),\n                       SalesByCity(2015, \u0026quot;Cambridge\u0026quot;, \u0026quot;MA\u0026quot;, \u0026quot;USA\u0026quot;, 3000),\n                       SalesByCity(2014, \u0026quot;Palo Alto\u0026quot;, \u0026quot;CA\u0026quot;, \u0026quot;USA\u0026quot;, 4000),\n                       SalesByCity(2015, \u0026quot;Palo Alto\u0026quot;, \u0026quot;CA\u0026quot;, \u0026quot;USA\u0026quot;, 6000),\n                       SalesByCity(2014, \u0026quot;Pune\u0026quot;, \u0026quot;MH\u0026quot;, \u0026quot;India\u0026quot;, 1000),\n                       SalesByCity(2015, \u0026quot;Pune\u0026quot;, \u0026quot;MH\u0026quot;, \u0026quot;India\u0026quot;, 1000),\n                       SalesByCity(2015, \u0026quot;Mumbai\u0026quot;, \u0026quot;MH\u0026quot;, \u0026quot;India\u0026quot;, 1000),\n                       SalesByCity(2014, \u0026quot;Mumbai\u0026quot;, \u0026quot;MH\u0026quot;, \u0026quot;India\u0026quot;, 2000))\n\nval salesByCityDF \u003d sc.parallelize(salesByCity).toDF()\nval rollup \u003d salesByCityDF.rollup($\u0026quot;country\u0026quot;, $\u0026quot;state\u0026quot;, $\u0026quot;city\u0026quot;).sum(\u0026quot;revenue\u0026quot;)\nrollup.show\n\n+-------+-----+---------+------------+\n|country|state|     city|sum(revenue)|\n+-------+-----+---------+------------+\n|  India|   MH|   Mumbai|      3000.0|\n|    USA|   MA|Cambridge|      5000.0|\n|  India|   MH|     Pune|      2000.0|\n|    USA|   MA|   Boston|      5000.0|\n|    USA|   MA|     null|     10000.0|\n|    USA| null|     null|     20000.0|\n|    USA|   CA|     null|     10000.0|\n|   null| null|     null|     25000.0|\n|  India|   MH|     null|      5000.0|\n|    USA|   CA|Palo Alto|     10000.0|\n|  India| null|     null|      5000.0|\n+-------+-----+---------+------------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003esample\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe sample method returns a DataFrame containing the specified fraction of the rows in the source DataFrame. It takes two arguments. The first argument is a Boolean value indicating whether sampling should be done with replacement. The second argument specifies the fraction of the rows that should be returned.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval sampleDF \u003d homeDF.sample(true, 0.10)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eselect\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe select method returns a DataFrame containing only the specified columns from the source DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval namesAgeDF \u003d customerDF.select(\u0026quot;name\u0026quot;, \u0026quot;age\u0026quot;)\nnamesAgeDF.show\n\n+--------+---+\n|    name|age|\n+--------+---+\n|   James| 21|\n|     Liz| 25|\n|    John| 31|\n|Jennifer| 45|\n|  Robert| 41|\n|  Sandra| 45|\n+--------+---+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA variant of the select method allows one or more Column expressions as arguments.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval newAgeDF \u003d customerDF.select($\u0026quot;name\u0026quot;, $\u0026quot;age\u0026quot; + 10)\nnewAgeDF.show\n\n+--------+----------+\n|    name|(age + 10)|\n+--------+----------+\n|   James|        31|\n|     Liz|        35|\n|    John|        41|\n|Jennifer|        55|\n|  Robert|        51|\n|  Sandra|        55|\n+--------+----------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eselectExpr\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe selectExpr method accepts one or more SQL expressions as arguments and returns a DataFrame generated by executing the specified SQL expressions.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval newCustomerDF \u003d customerDF.selectExpr(\u0026quot;name\u0026quot;, \u0026quot;age + 10  AS new_age\u0026quot;,\n                                          \u0026quot;IF(gender \u003d \u0026#39;M\u0026#39;, true, false) AS male\u0026quot;)\n\nnewCustomerDF.show\n\n+--------+-------+-----+\n|    name|new_age| male|\n+--------+-------+-----+\n|   James|     31| true|\n|     Liz|     35|false|\n|    John|     41| true|\n|Jennifer|     55|false|\n|  Robert|     51| true|\n|  Sandra|     55|false|\n+--------+-------+-----+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003ewithColumn\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe withColumn method adds a new column to or replaces an existing column in the source DataFrame and returns a new DataFrame. It takes two arguments. The first argument is the name of the new column and the second argument is an expression for generating the values of the new column.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval newProductDF \u003d productDF.withColumn(\u0026quot;profit\u0026quot;, $\u0026quot;price\u0026quot; - $\u0026quot;cost\u0026quot;)\nnewProductDF.show\n\n+---+-------+------+-----+------+\n|pId|   name| price| cost|profit|\n+---+-------+------+-----+------+\n|  1| iPhone| 600.0|400.0| 200.0|\n|  2| Galaxy| 500.0|400.0| 100.0|\n|  3|   iPad| 400.0|300.0| 100.0|\n|  4| Kindle| 200.0|100.0| 100.0|\n|  5|MacBook|1200.0|900.0| 300.0|\n|  6|   Dell| 500.0|400.0| 100.0|\n+---+-------+------+-----+------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488326340874_-654996589",
      "id": "20170301-005900_363428902",
      "dateCreated": "Mar 1, 2017 12:59:00 AM",
      "dateStarted": "Mar 1, 2017 12:33:06 PM",
      "dateFinished": "Mar 1, 2017 12:33:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nRDD Operations\n---\n\nThe DataFrame class supports commonly used RDD operations such as map, flatMap, foreach, foreachPartition, mapPartition, coalesce, and repartition. These methods work similar to their namesake operations in the RDD class.\n\nIn addition, if you need access to other RDD methods that are not present in the DataFrame class, you can get an RDD from a DataFrame. This section discusses the commonly used techniques for generating an RDD from a DataFrame.\n\n**rdd**\n\nrdd is defined as a lazy val in the DataFrame class. It represents the source DataFrame as an RDD of Row instances.\n\nAs discussed earlier, a Row represents a relational tuple in the source DataFrame. It allows both generic access and native primitive access of fields by their ordinal.\n\nAn example is shown next.\n~~~\nval rdd \u003d customerDF.rdd\n\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d MapPartitionsRDD[405] at rdd at \u003cconsole\u003e:27\n\nval firstRow \u003d rdd.first\n\nfirstRow: org.apache.spark.sql.Row \u003d [1,James,21,M]\n\nval name \u003d firstRow.getString(1)\n\nname: String \u003d James\n\nval age \u003d firstRow.getInt(2)\n\nage: Int \u003d 21\n~~~\nFields in a Row can also be extracted using Scala pattern matching.\n~~~\nimport org.apache.spark.sql.Row\nval rdd \u003d customerDF.rdd\n\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d MapPartitionsRDD[113] at rdd at \u003cconsole\u003e:28\n\nval nameAndAge \u003d rdd.map {\n                   case Row(cId: Long, name: String, age: Int, gender: String) \u003d\u003e (name, age)\n                 }\n\nnameAndAge: org.apache.spark.rdd.RDD[(String, Int)] \u003d MapPartitionsRDD[114] at map at \u003cconsole\u003e:30\n\nnameAndAge.collect\n\nres79: Array[(String, Int)] \u003d Array((James,21), (Liz,25), (John,31), (Jennifer,45), (Robert,41), (Sandra,45))\n~~~\n**toJSON**\n\nThe toJSON method generates an RDD of JSON strings from the source DataFrame. Each element in the returned RDD is a JSON object.\n~~~\nval jsonRDD \u003d customerDF.toJSON\n\njsonRDD: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[408] at toJSON at \u003cconsole\u003e:28\n\njsonRDD.collect\n\nres80: Array[String] \u003d Array({\"cId\":1,\"name\":\"James\",\"age\":21,\"gender\":\"M\"}, {\"cId\":2,\"name\":\"Liz\",\"age\":25,\"gender\":\"F\"}, {\"cId\":3,\"name\":\"John\",\"age\":31,\"gender\":\"M\"}, {\"cId\":4,\"name\":\"Jennifer\",\"age\":45,\"gender\":\"F\"}, {\"cId\":5,\"name\":\"Robert\",\"age\":41,\"gender\":\"M\"}, {\"cId\":6,\"name\":\"Sandra\",\"age\":45,\"gender\":\"F\"})\n~~~\n**Actions**\n\nSimilar to the RDD actions, the action methods in the DataFrame class return results to the Driver program. This section covers the commonly used action methods in the DataFrame class.\n\n**collect**\n\nThe collect method returns the data in a DataFrame as an array of Rows.\n~~~\nval result \u003d customerDF.collect\n\nresult: Array[org.apache.spark.sql.Row] \u003d Array([1,James,21,M], [2,Liz,25,F], [3,John,31,M], [4,Jennifer,45,F], [5,Robert,41,M], [6,Sandra,45,F])\n~~~\n**count**\n\nThe count method returns the number of rows in the source DataFrame.\n~~~\nval count \u003d customerDF.count\n\ncount: Long \u003d 6\n~~~\n**describe**\n\nThe describe method can be used for exploratory data analysis. It returns summary statistics for numeric columns in the source DataFrame. The summary statistics includes min, max, count, mean, and standard deviation. It takes the names of one or more columns as arguments.\n~~~\nval summaryStatsDF \u003d productDF.describe(\"price\", \"cost\")\n\nsummaryStatsDF: org.apache.spark.sql.DataFrame \u003d [summary: string, price: string, cost: string]\n\nsummaryStatsDF.show\n\n+-------+------------------+------------------+\n|summary|             price|              cost|\n+-------+------------------+------------------+\n|  count|                 6|                 6|\n|   mean| 566.6666666666666| 416.6666666666667|\n| stddev|309.12061651652357|240.94720491334928|\n|    min|             200.0|             100.0|\n|    max|            1200.0|             900.0|\n+-------+------------------+------------------+\n~~~\n**first**\n\nThe first method returns the first row in the source DataFrame.\n~~~\nval first \u003d customerDF.first\n\nfirst: org.apache.spark.sql.Row \u003d [1,James,21,M]\n~~~\n**show**\n\nThe show method displays the rows in the source DataFrame on the driver console in a tabular format. It optionally takes an integer N as an argument and displays the top N rows. If no argument is provided, it shows the top 20 rows.\n~~~\ncustomerDF.show(2)\n\n+---+-----+---+------+\n|cId| name|age|gender|\n+---+-----+---+------+\n|  1|James| 21|     M|\n|  2|  Liz| 25|     F|\n+---+-----+---+------+\nonly showing top 2 rows\n~~~\n**take**\n\nThe take method takes an integer N as an argument and returns the first N rows from the source DataFrame as an array of Rows.\n~~~\nval first2Rows \u003d customerDF.take(2)\n\nfirst2Rows: Array[org.apache.spark.sql.Row] \u003d Array([1,James,21,M], [2,Liz,25,F])\n~~~\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 2:05:48 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eRDD Operations\u003c/h2\u003e\n\u003cp\u003eThe DataFrame class supports commonly used RDD operations such as map, flatMap, foreach, foreachPartition, mapPartition, coalesce, and repartition. These methods work similar to their namesake operations in the RDD class.\u003c/p\u003e\n\u003cp\u003eIn addition, if you need access to other RDD methods that are not present in the DataFrame class, you can get an RDD from a DataFrame. This section discusses the commonly used techniques for generating an RDD from a DataFrame.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003erdd\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003erdd is defined as a lazy val in the DataFrame class. It represents the source DataFrame as an RDD of Row instances.\u003c/p\u003e\n\u003cp\u003eAs discussed earlier, a Row represents a relational tuple in the source DataFrame. It allows both generic access and native primitive access of fields by their ordinal.\u003c/p\u003e\n\u003cp\u003eAn example is shown next.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval rdd \u003d customerDF.rdd\n\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d MapPartitionsRDD[405] at rdd at \u0026lt;console\u0026gt;:27\n\nval firstRow \u003d rdd.first\n\nfirstRow: org.apache.spark.sql.Row \u003d [1,James,21,M]\n\nval name \u003d firstRow.getString(1)\n\nname: String \u003d James\n\nval age \u003d firstRow.getInt(2)\n\nage: Int \u003d 21\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFields in a Row can also be extracted using Scala pattern matching.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark.sql.Row\nval rdd \u003d customerDF.rdd\n\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d MapPartitionsRDD[113] at rdd at \u0026lt;console\u0026gt;:28\n\nval nameAndAge \u003d rdd.map {\n                   case Row(cId: Long, name: String, age: Int, gender: String) \u003d\u0026gt; (name, age)\n                 }\n\nnameAndAge: org.apache.spark.rdd.RDD[(String, Int)] \u003d MapPartitionsRDD[114] at map at \u0026lt;console\u0026gt;:30\n\nnameAndAge.collect\n\nres79: Array[(String, Int)] \u003d Array((James,21), (Liz,25), (John,31), (Jennifer,45), (Robert,41), (Sandra,45))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003etoJSON\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe toJSON method generates an RDD of JSON strings from the source DataFrame. Each element in the returned RDD is a JSON object.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval jsonRDD \u003d customerDF.toJSON\n\njsonRDD: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[408] at toJSON at \u0026lt;console\u0026gt;:28\n\njsonRDD.collect\n\nres80: Array[String] \u003d Array({\u0026quot;cId\u0026quot;:1,\u0026quot;name\u0026quot;:\u0026quot;James\u0026quot;,\u0026quot;age\u0026quot;:21,\u0026quot;gender\u0026quot;:\u0026quot;M\u0026quot;}, {\u0026quot;cId\u0026quot;:2,\u0026quot;name\u0026quot;:\u0026quot;Liz\u0026quot;,\u0026quot;age\u0026quot;:25,\u0026quot;gender\u0026quot;:\u0026quot;F\u0026quot;}, {\u0026quot;cId\u0026quot;:3,\u0026quot;name\u0026quot;:\u0026quot;John\u0026quot;,\u0026quot;age\u0026quot;:31,\u0026quot;gender\u0026quot;:\u0026quot;M\u0026quot;}, {\u0026quot;cId\u0026quot;:4,\u0026quot;name\u0026quot;:\u0026quot;Jennifer\u0026quot;,\u0026quot;age\u0026quot;:45,\u0026quot;gender\u0026quot;:\u0026quot;F\u0026quot;}, {\u0026quot;cId\u0026quot;:5,\u0026quot;name\u0026quot;:\u0026quot;Robert\u0026quot;,\u0026quot;age\u0026quot;:41,\u0026quot;gender\u0026quot;:\u0026quot;M\u0026quot;}, {\u0026quot;cId\u0026quot;:6,\u0026quot;name\u0026quot;:\u0026quot;Sandra\u0026quot;,\u0026quot;age\u0026quot;:45,\u0026quot;gender\u0026quot;:\u0026quot;F\u0026quot;})\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eActions\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSimilar to the RDD actions, the action methods in the DataFrame class return results to the Driver program. This section covers the commonly used action methods in the DataFrame class.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ecollect\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe collect method returns the data in a DataFrame as an array of Rows.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval result \u003d customerDF.collect\n\nresult: Array[org.apache.spark.sql.Row] \u003d Array([1,James,21,M], [2,Liz,25,F], [3,John,31,M], [4,Jennifer,45,F], [5,Robert,41,M], [6,Sandra,45,F])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003ecount\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe count method returns the number of rows in the source DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval count \u003d customerDF.count\n\ncount: Long \u003d 6\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003edescribe\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe describe method can be used for exploratory data analysis. It returns summary statistics for numeric columns in the source DataFrame. The summary statistics includes min, max, count, mean, and standard deviation. It takes the names of one or more columns as arguments.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval summaryStatsDF \u003d productDF.describe(\u0026quot;price\u0026quot;, \u0026quot;cost\u0026quot;)\n\nsummaryStatsDF: org.apache.spark.sql.DataFrame \u003d [summary: string, price: string, cost: string]\n\nsummaryStatsDF.show\n\n+-------+------------------+------------------+\n|summary|             price|              cost|\n+-------+------------------+------------------+\n|  count|                 6|                 6|\n|   mean| 566.6666666666666| 416.6666666666667|\n| stddev|309.12061651652357|240.94720491334928|\n|    min|             200.0|             100.0|\n|    max|            1200.0|             900.0|\n+-------+------------------+------------------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003efirst\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe first method returns the first row in the source DataFrame.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval first \u003d customerDF.first\n\nfirst: org.apache.spark.sql.Row \u003d [1,James,21,M]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eshow\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe show method displays the rows in the source DataFrame on the driver console in a tabular format. It optionally takes an integer N as an argument and displays the top N rows. If no argument is provided, it shows the top 20 rows.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.show(2)\n\n+---+-----+---+------+\n|cId| name|age|gender|\n+---+-----+---+------+\n|  1|James| 21|     M|\n|  2|  Liz| 25|     F|\n+---+-----+---+------+\nonly showing top 2 rows\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003etake\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe take method takes an integer N as an argument and returns the first N rows from the source DataFrame as an array of Rows.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval first2Rows \u003d customerDF.take(2)\n\nfirst2Rows: Array[org.apache.spark.sql.Row] \u003d Array([1,James,21,M], [2,Liz,25,F])\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488329060646_-1912250369",
      "id": "20170301-014420_1842446467",
      "dateCreated": "Mar 1, 2017 1:44:20 AM",
      "dateStarted": "Mar 1, 2017 2:05:44 AM",
      "dateFinished": "Mar 1, 2017 2:05:44 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nOutput Operations\n---\nAn output operation saves a DataFrame to a storage system. Prior to version 1.4, DataFrame included a number of different methods for saving a DataFrame to a variety of storage systems. Starting with version 1.4, those methods were replaced by the write method.\n\n**write**\n\nThe write method returns an instance of the DataFrameWriter class, which provides methods for saving the contents of a DataFrame to a data source. The next section covers the DataFrameWriter class.\n\n**Saving a DataFrame**\n\nSpark SQL provides a unified interface for saving a DataFrame to a variety of data sources. The same interface can be used to write data to relational databases, NoSQL data stores and a variety of file formats.\n\nThe DataFrameWriter class defines the interface for writing data to a data source. Through its builder methods, it allows you to specify different options for saving data. For example, you can specify format, partitioning, and handling of existing data.\n\nThe following examples show how to save a DataFrame to different storage systems.\n~~~\n// save a DataFrame in JSON format\ncustomerDF.write\n  .format(\"org.apache.spark.sql.json\")\n  .save(\"path/to/output-directory\")\n\n// save a DataFrame in Parquet format\nhomeDF.write\n  .format(\"org.apache.spark.sql.parquet\")\n  .partitionBy(\"city\")\n  .save(\"path/to/output-directory\")\n\n// save a DataFrame in ORC file format\nhomeDF.write\n  .format(\"orc\")\n  .partitionBy(\"city\")\n  .save(\"path/to/output-directory\")\n\n// save a DataFrame as a Postgres database table\ndf.write\n  .format(\"org.apache.spark.sql.jdbc\")\n  .options(Map(\n     \"url\" -\u003e \"jdbc:postgresql://host:port/database?user\u003d\u003cUSER\u003e\u0026password\u003d\u003cPASS\u003e\",\n     \"dbtable\" -\u003e \"schema-name.table-name\"))\n   .save()\n\n// save a DataFrame to a Hive table\ndf.write.saveAsTable(\"hive-table-name\")\n~~~\nYou can save a DataFrame in Parquet, JSON, ORC, or CSV format to any Hadoop-supported storage system, including local file system, HDFS or Amazon S3.\n\nIf a data source supports partitioned layout, the DataFrameWriter class supports it through the partitionBy method. It will partition the rows by the specified column and create a separate subdirectory for each unique value in the specified column. Partitioned layout enables partition pruning during query. Future queries through Spark SQL will be able to skip large amounts of disk I/O when a partitioned column is referenced in a predicate.\n\nConsider the following example.\n~~~\nhomeDF.write\n  .format(\"parquet\")\n  .partitionBy(\"city\")\n  .save(\"homes\")\n~~~\nThe preceding code partitions rows by the city column. A subdirectory will be created for each unique value of city. For example, subdirectories named city\u003dBerkeley, city\u003dFremont, city\u003dOakland, and so on, will be created under the homes directory.\n\nThe following code will only read the subdirectory named city\u003dBerkeley and skip all other subdirectories under the homes directory. For a large dataset that includes data for hundreds of cities, this could speed up application performance by orders of magnitude.\n~~~\nval newHomesDF \u003d sqlContext.read.format(\"parquet\").load(\"homes\")\nnewHomesDF.registerTempTable(\"homes\")\nval homesInBerkeley \u003d sqlContext.sql(\"SELECT * FROM homes WHERE city \u003d \u0027Berkeley\u0027\")\n~~~\nWhile saving a DataFrame, if the destination path or table already exists, Spark SQL will throw an exception by default. You can change this behavior by calling the mode method in the DataFrameWriter class. It takes an argument, which specifies the behavior if destination path or table already exists. The mode method supports the following options:\n~~~\n    error (default) - throw an exception if destination path/table already exists.\n    append - append to existing data if destination path/table already exists.\n    overwrite - overwrite existing data if destination path/table exists.\n    ignore - ignore the operation if destination path/table exists.\n~~~\nA few examples are shown next.\n~~~\ncustomerDF.write\n  .format(\"parquet\")\n  .mode(\"overwrite\")\n  .save(\"path/to/output-directory\")\n\ncustomerDF.write\n  .mode(\"append\")\n  .saveAsTable(\"hive-table-name\")\n~~~\nIn addition to the methods shown here for writing data to a data source, the DataFrameWriter class provides special methods for writing data to the data sources for which it has built-in support for. These data sources include Parquet, ORC, JSON, Hive, and JDBC-compliant databases.\n\n**JSON**\n\nThe json method saves the contents of a DataFrame in JSON format. It takes as argument a path, which can be on a local file system, HDFS or S3.\n~~~\ncustomerDF.write.json(\"path/to/directory\")\n~~~\n**Parquet**\n\nThe parquet method saves the contents of a DataFrame in Parquet format. It takes a path as argument and saves a DataFrame at the specified path.\n~~~\ncustomerDF.write.parquet(\"path/to/directory\")\n~~~\n**ORC**\n\nThe orc method saves a DataFrame in ORC file format. Similar to the JSON and Parquet methods, it takes a path as argument.\n~~~\ncustomerDF.write.orc(\"path/to/directory\")\n~~~\n**Hive**\n\nThe saveAsTable method saves the content of a DataFrame as a Hive table. It saves a DataFrame to a file and registers metadata as a table in Hive metastore.\n~~~\ncustomerDF.write.saveAsTable(\"hive-table-name\")\n~~~\n**JDBC-Compliant Database**\n\nThe jdbc method saves a DataFrame to a database using the JDBC interface. It takes three arguments: JDBC URL of a database, table name, and connection properties. The connection properties specify connection arguments such as user name and password.\n~~~\nval jdbcUrl \u003d\"jdbc:mysql://host:port/database\"\nval tableName \u003d \"table-name\"\nval connectionProperties \u003d new java.util.Properties\nconnectionProperties.setProperty(\"user\",\"database-user-name\")\nconnectionProperties.setProperty(\"password\",\" database-user-password\")\n\ncustomerDF.write.jdbc(jdbcUrl, tableName, connectionProperties)\n\n~~~",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 1:25:24 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eOutput Operations\u003c/h2\u003e\n\u003cp\u003eAn output operation saves a DataFrame to a storage system. Prior to version 1.4, DataFrame included a number of different methods for saving a DataFrame to a variety of storage systems. Starting with version 1.4, those methods were replaced by the write method.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ewrite\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe write method returns an instance of the DataFrameWriter class, which provides methods for saving the contents of a DataFrame to a data source. The next section covers the DataFrameWriter class.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSaving a DataFrame\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpark SQL provides a unified interface for saving a DataFrame to a variety of data sources. The same interface can be used to write data to relational databases, NoSQL data stores and a variety of file formats.\u003c/p\u003e\n\u003cp\u003eThe DataFrameWriter class defines the interface for writing data to a data source. Through its builder methods, it allows you to specify different options for saving data. For example, you can specify format, partitioning, and handling of existing data.\u003c/p\u003e\n\u003cp\u003eThe following examples show how to save a DataFrame to different storage systems.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e// save a DataFrame in JSON format\ncustomerDF.write\n  .format(\u0026quot;org.apache.spark.sql.json\u0026quot;)\n  .save(\u0026quot;path/to/output-directory\u0026quot;)\n\n// save a DataFrame in Parquet format\nhomeDF.write\n  .format(\u0026quot;org.apache.spark.sql.parquet\u0026quot;)\n  .partitionBy(\u0026quot;city\u0026quot;)\n  .save(\u0026quot;path/to/output-directory\u0026quot;)\n\n// save a DataFrame in ORC file format\nhomeDF.write\n  .format(\u0026quot;orc\u0026quot;)\n  .partitionBy(\u0026quot;city\u0026quot;)\n  .save(\u0026quot;path/to/output-directory\u0026quot;)\n\n// save a DataFrame as a Postgres database table\ndf.write\n  .format(\u0026quot;org.apache.spark.sql.jdbc\u0026quot;)\n  .options(Map(\n     \u0026quot;url\u0026quot; -\u0026gt; \u0026quot;jdbc:postgresql://host:port/database?user\u003d\u0026lt;USER\u0026gt;\u0026amp;password\u003d\u0026lt;PASS\u0026gt;\u0026quot;,\n     \u0026quot;dbtable\u0026quot; -\u0026gt; \u0026quot;schema-name.table-name\u0026quot;))\n   .save()\n\n// save a DataFrame to a Hive table\ndf.write.saveAsTable(\u0026quot;hive-table-name\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can save a DataFrame in Parquet, JSON, ORC, or CSV format to any Hadoop-supported storage system, including local file system, HDFS or Amazon S3.\u003c/p\u003e\n\u003cp\u003eIf a data source supports partitioned layout, the DataFrameWriter class supports it through the partitionBy method. It will partition the rows by the specified column and create a separate subdirectory for each unique value in the specified column. Partitioned layout enables partition pruning during query. Future queries through Spark SQL will be able to skip large amounts of disk I/O when a partitioned column is referenced in a predicate.\u003c/p\u003e\n\u003cp\u003eConsider the following example.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ehomeDF.write\n  .format(\u0026quot;parquet\u0026quot;)\n  .partitionBy(\u0026quot;city\u0026quot;)\n  .save(\u0026quot;homes\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe preceding code partitions rows by the city column. A subdirectory will be created for each unique value of city. For example, subdirectories named city\u003dBerkeley, city\u003dFremont, city\u003dOakland, and so on, will be created under the homes directory.\u003c/p\u003e\n\u003cp\u003eThe following code will only read the subdirectory named city\u003dBerkeley and skip all other subdirectories under the homes directory. For a large dataset that includes data for hundreds of cities, this could speed up application performance by orders of magnitude.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval newHomesDF \u003d sqlContext.read.format(\u0026quot;parquet\u0026quot;).load(\u0026quot;homes\u0026quot;)\nnewHomesDF.registerTempTable(\u0026quot;homes\u0026quot;)\nval homesInBerkeley \u003d sqlContext.sql(\u0026quot;SELECT * FROM homes WHERE city \u003d \u0026#39;Berkeley\u0026#39;\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhile saving a DataFrame, if the destination path or table already exists, Spark SQL will throw an exception by default. You can change this behavior by calling the mode method in the DataFrameWriter class. It takes an argument, which specifies the behavior if destination path or table already exists. The mode method supports the following options:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e    error (default) - throw an exception if destination path/table already exists.\n    append - append to existing data if destination path/table already exists.\n    overwrite - overwrite existing data if destination path/table exists.\n    ignore - ignore the operation if destination path/table exists.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA few examples are shown next.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.write\n  .format(\u0026quot;parquet\u0026quot;)\n  .mode(\u0026quot;overwrite\u0026quot;)\n  .save(\u0026quot;path/to/output-directory\u0026quot;)\n\ncustomerDF.write\n  .mode(\u0026quot;append\u0026quot;)\n  .saveAsTable(\u0026quot;hive-table-name\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn addition to the methods shown here for writing data to a data source, the DataFrameWriter class provides special methods for writing data to the data sources for which it has built-in support for. These data sources include Parquet, ORC, JSON, Hive, and JDBC-compliant databases.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eJSON\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe json method saves the contents of a DataFrame in JSON format. It takes as argument a path, which can be on a local file system, HDFS or S3.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.write.json(\u0026quot;path/to/directory\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eParquet\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe parquet method saves the contents of a DataFrame in Parquet format. It takes a path as argument and saves a DataFrame at the specified path.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.write.parquet(\u0026quot;path/to/directory\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eORC\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe orc method saves a DataFrame in ORC file format. Similar to the JSON and Parquet methods, it takes a path as argument.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.write.orc(\u0026quot;path/to/directory\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eHive\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe saveAsTable method saves the content of a DataFrame as a Hive table. It saves a DataFrame to a file and registers metadata as a table in Hive metastore.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecustomerDF.write.saveAsTable(\u0026quot;hive-table-name\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eJDBC-Compliant Database\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe jdbc method saves a DataFrame to a database using the JDBC interface. It takes three arguments: JDBC URL of a database, table name, and connection properties. The connection properties specify connection arguments such as user name and password.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval jdbcUrl \u003d\u0026quot;jdbc:mysql://host:port/database\u0026quot;\nval tableName \u003d \u0026quot;table-name\u0026quot;\nval connectionProperties \u003d new java.util.Properties\nconnectionProperties.setProperty(\u0026quot;user\u0026quot;,\u0026quot;database-user-name\u0026quot;)\nconnectionProperties.setProperty(\u0026quot;password\u0026quot;,\u0026quot; database-user-password\u0026quot;)\n\ncustomerDF.write.jdbc(jdbcUrl, tableName, connectionProperties)\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488330100561_723753525",
      "id": "20170301-020140_719756754",
      "dateCreated": "Mar 1, 2017 2:01:40 AM",
      "dateStarted": "Mar 1, 2017 1:25:24 PM",
      "dateFinished": "Mar 1, 2017 1:25:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nBuilt-in Functions\n---\n\nSpark SQL comes with a comprehensive list of built-in functions, which are optimized for fast execution. It implements these functions with code generation techniques. The built-in functions can be used from both the DataFrame API and SQL interface.\n\nTo use Spark’s built-in functions from the DataFrame API, you need to add the following import statement to your source code.\n\nimport org.apache.spark.sql.functions._\n\nThe built-in functions can be classified into the following categories: aggregate, collection, date/time, math, string, window, and miscellaneous functions.\n\n**Aggregate**\n\nThe aggregate functions can be used to perform aggregations on a column. The built-in aggregate functions include approxCountDistinct, avg, count, countDistinct, first, last, max, mean, min, sum, and sumDistinct.\n\nThe following example illustrates how you can use a built-in function.\n~~~\nval minPrice \u003d homeDF.select(min($\"price\"))\nminPrice.show\n\n+----------+\n|min(price)|\n+----------+\n|   1100000|\n+----------+\n~~~\n**Collection**\n\nThe collection functions operate on columns containing a collection of elements. The built-in collection functions include array_contains, explode, size, and sort_array.\n\n**Date/Time**\n\nThe date/time functions make it easy to process columns containing date/time values. These functions can be further sub-classified into the following categories: conversion, extraction, arithmetic, and miscellaneous functions.\n\n**Conversion**\n\nThe conversion functions convert date/time values from one format to another. For example, you can convert a timestamp string in yyyy-MM-dd HH:mm:ss format to a Unix epoch value using the unix_timestamp function. Conversely, the from_unixtime function converts a Unix epoch value to a string representation. Spark SQL also provides functions for converting timestamps from one time zone to another.\n\nThe built-in conversion functions include unix_timestamp, from_unixtime, to_date, quarter, day, dayofyear, weekofyear, from_utc_timestamp, and to_utc_timestamp.\n\n**Field Extraction**\n\nThe field extraction functions allow you to extract year, month, day, hour, minute, and second from a Date/Time value. The built-in field extraction functions include year, quarter, month, weekofyear, dayofyear, dayofmonth, hour, minute, and second.\n\n**Date Arithmetic**\n\nThe arithmetic functions allow you to perform arithmetic operation on columns containing dates. For example, you can calculate the difference between two dates, add days to a date, or subtract days from a date. The built-in date arithmetic functions include datediff, date_add, date_sub, add_months, last_day, next_day, and months_between.\n\n**Miscellaneous**\n\nIn addition to the functions mentioned earlier, Spark SQL provides a few other useful date- and time-related functions, such as current_date, current_timestamp, trunc, date_format.\n\n**Math**\n\nThe math functions operate on columns containing numerical values. Spark SQL comes with a long list of built-in math functions. Example include abs, ceil, cos, exp, factorial, floor, hex, hypot, log, log10, pow, round, shiftLeft, sin, sqrt, tan, and other commonly used math functions.\n\n**String**\n\nSpark SQL provides a variety of built-in functions for processing columns that contain string values. For example, you can split, trim or change case of a string. The built-in string functions include ascii, base64, concat, concat_ws, decode, encode, format_number, format_string, get_json_object, initcap, instr, length, levenshtein, locate, lower, lpad, ltrim, printf, regexp_extract, regexp_replace, repeat, reverse, rpad, rtrim, soundex, space, split, substring, substring_index, translate, trim, unbase64, upper, and other commonly used string functions.\n\n**Window**\n\nSpark SQL supports window functions for analytics. A window function performs a calculation across a set of rows that are related to the current row. The built-in window functions provided by Spark SQL include cumeDist, denseRank, lag, lead, ntile, percentRank, rank, and rowNumber.\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 2:15:05 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eBuilt-in Functions\u003c/h2\u003e\n\u003cp\u003eSpark SQL comes with a comprehensive list of built-in functions, which are optimized for fast execution. It implements these functions with code generation techniques. The built-in functions can be used from both the DataFrame API and SQL interface.\u003c/p\u003e\n\u003cp\u003eTo use Spark’s built-in functions from the DataFrame API, you need to add the following import statement to your source code.\u003c/p\u003e\n\u003cp\u003eimport org.apache.spark.sql.functions._\u003c/p\u003e\n\u003cp\u003eThe built-in functions can be classified into the following categories: aggregate, collection, date/time, math, string, window, and miscellaneous functions.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAggregate\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe aggregate functions can be used to perform aggregations on a column. The built-in aggregate functions include approxCountDistinct, avg, count, countDistinct, first, last, max, mean, min, sum, and sumDistinct.\u003c/p\u003e\n\u003cp\u003eThe following example illustrates how you can use a built-in function.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval minPrice \u003d homeDF.select(min($\u0026quot;price\u0026quot;))\nminPrice.show\n\n+----------+\n|min(price)|\n+----------+\n|   1100000|\n+----------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eCollection\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe collection functions operate on columns containing a collection of elements. The built-in collection functions include array_contains, explode, size, and sort_array.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDate/Time\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe date/time functions make it easy to process columns containing date/time values. These functions can be further sub-classified into the following categories: conversion, extraction, arithmetic, and miscellaneous functions.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eConversion\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe conversion functions convert date/time values from one format to another. For example, you can convert a timestamp string in yyyy-MM-dd HH:mm:ss format to a Unix epoch value using the unix_timestamp function. Conversely, the from_unixtime function converts a Unix epoch value to a string representation. Spark SQL also provides functions for converting timestamps from one time zone to another.\u003c/p\u003e\n\u003cp\u003eThe built-in conversion functions include unix_timestamp, from_unixtime, to_date, quarter, day, dayofyear, weekofyear, from_utc_timestamp, and to_utc_timestamp.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eField Extraction\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe field extraction functions allow you to extract year, month, day, hour, minute, and second from a Date/Time value. The built-in field extraction functions include year, quarter, month, weekofyear, dayofyear, dayofmonth, hour, minute, and second.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDate Arithmetic\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe arithmetic functions allow you to perform arithmetic operation on columns containing dates. For example, you can calculate the difference between two dates, add days to a date, or subtract days from a date. The built-in date arithmetic functions include datediff, date_add, date_sub, add_months, last_day, next_day, and months_between.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMiscellaneous\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn addition to the functions mentioned earlier, Spark SQL provides a few other useful date- and time-related functions, such as current_date, current_timestamp, trunc, date_format.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMath\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe math functions operate on columns containing numerical values. Spark SQL comes with a long list of built-in math functions. Example include abs, ceil, cos, exp, factorial, floor, hex, hypot, log, log10, pow, round, shiftLeft, sin, sqrt, tan, and other commonly used math functions.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eString\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpark SQL provides a variety of built-in functions for processing columns that contain string values. For example, you can split, trim or change case of a string. The built-in string functions include ascii, base64, concat, concat_ws, decode, encode, format_number, format_string, get_json_object, initcap, instr, length, levenshtein, locate, lower, lpad, ltrim, printf, regexp_extract, regexp_replace, repeat, reverse, rpad, rtrim, soundex, space, split, substring, substring_index, translate, trim, unbase64, upper, and other commonly used string functions.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWindow\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpark SQL supports window functions for analytics. A window function performs a calculation across a set of rows that are related to the current row. The built-in window functions provided by Spark SQL include cumeDist, denseRank, lag, lead, ntile, percentRank, rank, and rowNumber.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488330683253_1448263063",
      "id": "20170301-021123_2123086585",
      "dateCreated": "Mar 1, 2017 2:11:23 AM",
      "dateStarted": "Mar 1, 2017 2:15:05 AM",
      "dateFinished": "Mar 1, 2017 2:15:06 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nnewHomesDF.registerTempTable(\"homes\")\n",
      "user": "anonymous",
      "dateUpdated": "Mar 2, 2017 10:13:25 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1488445674172_-1380240394",
      "id": "20170302-100754_541203780",
      "dateCreated": "Mar 2, 2017 10:07:54 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nUDFs and UDAFs\n---\n\n\nSpark SQL allows user-defined functions (UDFs) and user-defined aggregation functions (UDAFs). Both UDFs and UDAFs perform custom computations on a dataset. A **UDF performs custom computation one row at a time and returns a value for each row**. A **UDAF applies custom aggregation over groups of rows**.\n\nUDFs and UDAFs can be used just like the built-in functions after they have been registered with Spark SQL.\n\nWriting a UDF is as easy as: \n~~~\n  def setupUDFs(sqlCtx: SQLContext) \u003d {\n    sqlCtx.udf.register(\"strLen\", (s: String) \u003d\u003e s.length())\n  }\n~~~\n\nAn UDAF is a little tricky:\n\n~~~\n  def setupUDAFs(sqlCtx: SQLContext) \u003d {\n    class Avg extends UserDefinedAggregateFunction {\n      // Input type\n      def inputSchema: org.apache.spark.sql.types.StructType \u003d\n        StructType(StructField(\"value\", DoubleType) :: Nil)\n\n      def bufferSchema: StructType \u003d StructType(\n        StructField(\"count\", LongType) ::\n        StructField(\"sum\", DoubleType) :: Nil\n      )\n\n      // Return type\n      def dataType: DataType \u003d DoubleType\n\n      def deterministic: Boolean \u003d true\n\n      def initialize(buffer: MutableAggregationBuffer): Unit \u003d {\n        buffer(0) \u003d 0L\n        buffer(1) \u003d 0.0\n      }\n\n      def update(buffer: MutableAggregationBuffer,input: Row): Unit \u003d {\n        buffer(0) \u003d buffer.getAs[Long](0) + 1\n        buffer(1) \u003d buffer.getAs[Double](1) + input.getAs[Double](0)\n      }\n\n      def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit \u003d {\n        buffer1(0) \u003d buffer1.getAs[Long](0) + buffer2.getAs[Long](0)\n        buffer1(1) \u003d buffer1.getAs[Double](1) + buffer2.getAs[Double](1)\n      }\n\n      def evaluate(buffer: Row): Any \u003d {\n        buffer.getDouble(1) / buffer.getLong(0)\n      }\n    }\n    // Optionally register\n    val avg \u003d new Avg\n    sqlCtx.udf.register(\"ourAvg\", avg)\n  }\n~~~\n\n**Interactive Analysis Example**\n\nAs you have seen in previous sections, Spark SQL can be used as an interactive data analysis tool. The previous sections used toy examples so that they were easy to understand. This section shows how Spark SQL can be used for interactive data analysis on a real-world dataset. You can use either language integrated queries or SQL/HiveQL for data analysis. This section demonstrates how they can be used interchangeably.\n\nThe dataset that will be used is the Yelp challenge dataset. You can download it from www.yelp.com/dataset_challenge. It includes a number of data files. The example uses the yelp_academic_dataset_business.json file, which contains information about businesses. It contains a JSON object per line. Each JSON object contains information about a business, including its name, city, state, reviews, average rating, category and other attributes. You can learn more details about the dataset on Yelp’s website.\n\nLet’s launch the Spark shell from a terminal, if it is not already running.\n~~~\npath/to/spark/bin/spark-shell --master local[*]\n~~~\nAs discussed previously, the --master argument specifies the Spark master to which the Spark shell should connect. For the examples in this section, you can use Spark in local mode. If you have a real Spark cluster, change the master URL argument to point to your Spark master. The rest of the code in this section does not require any change.\n\nFor readability, I have split some of the example code statements into multiple lines. However, the Spark shell executes a statement as soon as you press the ENTER key. For multiline code statements, you can use Spark shell’s paste mode (:paste). Alternatively, enter a complete statement on a single line.\n\nConsider the following example.\n~~~\nbiz.filter(\"...\")\n   .select(\"...\")\n   .show()\n~~~\nIn the Spark shell, type it without the line breaks, as shown next.\n~~~\nbiz.filter(\"...\").select(\"...\").show()\n~~~\nConsider another example.\n~~~\nsqlContext.sql(\"SELECT x, count(y) as total FROM t\n                GROUP BY x\n                ORDER BY total\")\n           .show(50)\n~~~\nType the preceding code in the Spark shell without line breaks, as shown next.\n~~~\nsqlContext.sql(\"SELECT x, count(y) as total FROM t GROUP BY x ORDER BY total \").show(50)\n~~~\nLet’s get started now.\n\nSince you will be using a few classes and functions from the Spark SQL library, you need the following import statement.\n~~~\nimport org.apache.spark.sql._\n~~~\nLet’s create a DataFrame from the Yelp businesses dataset.\n~~~\nval biz \u003d sqlContext.read.json(\"path/to/yelp_academic_dataset_business.json\")\n~~~\nThe preceding statement is equivalent to this:\n~~~\nval biz \u003d sqlContext.read.format(\"json\").load(\"path/to/yelp_academic_dataset_business.json\")\n~~~\nMake sure that you specify the correct file path. Replace “path/to” with the name of the directory where you unpacked the Yelp dataset. Spark SQL will throw an exception if it cannot find the file at the specified path.\n\nSpark SQL reads the entire dataset once to infer the schema from a JSON file. Let’s check the schema inferred by Spark SQL.\n~~~\nbiz.printSchema()\n\nPartial output is shown next.\n\nroot\n |-- attributes: struct (nullable \u003d true)\n |    |-- Accepts Credit Cards: string (nullable \u003d true)\n |    |-- Ages Allowed: string (nullable \u003d true)\n |    |-- Alcohol: string (nullable \u003d true)\n ...\n ...\n ...\n |-- name: string (nullable \u003d true)\n |-- open: boolean (nullable \u003d true)\n |-- review_count: long (nullable \u003d true)\n |-- stars: double (nullable \u003d true)\n |-- city: string (nullable \u003d true)\n |-- state: string (nullable \u003d true)\n |-- type: string (nullable \u003d true)\n~~~\nTo use the SQL/HiveQL interface, you need to register the biz DataFrame as a temporary table.\n~~~\nbiz.registerTempTable(\"biz\")\n~~~\nNow you can analyze the Yelp businesses dataset using either the DataFrame API or SQL/HiveQL. I will show the code for both, but show the output only for the SQL/HiveQL queries.\n\nLet’s cache the data in memory since you will be querying it more than one once.\n\n**Language-Integrated Query**\n~~~\nbiz.cache()\n~~~\n**SQL**\n~~~\nsqlContext.cacheTable(\"biz\")\n~~~\nYou can also cache a table using the CACHE TABLE statement, as shown next.\n~~~\nsqlContext.sql(\"CACHE TABLE biz\")\n~~~\nNote that unlike RDD caching, Spark SQL immediately caches a table when you use the CACHE TABLE statement.\n\nSince the goal of this section is to just demonstrate how you can use the Spark SQL library for interactive data analysis, the steps from this point onward have no specific ordering. I chose a few seemingly random queries. You may analyze data in a different sequence. Before each example, I will state the goal and then show the Spark SQL code for achieving that goal.\n\nLet’s find the number of businesses in the dataset.\n\n**Language-Integrated Query**\n~~~\nval count \u003d biz.count()\n~~~\n\n**SQL**\n~~~\nsqlContext.sql(\"SELECT count(1) as businesses FROM biz\").show\n\n+----------+\n|businesses|\n+----------+\n|     61184|\n+----------+\n~~~\nNext, let’s find the count of businesses by state.\n\n**Language-Integrated Query**\n~~~\nval bizCountByState \u003d biz.groupBy(\"state\").count\nbizCountByState.show(50)\n~~~\n**SQL**\n~~~\nsqlContext.sql(\"SELECT state, count(1) as businesses FROM biz GROUP BY state\").show(50)\n\n+-----+----------+\n|state|businesses|\n+-----+----------+\n|  XGL|         1|\n|   NC|      4963|\n|   NV|     16485|\n|   AZ|     25230|\n...\n...\n|  MLN|       123|\n|  NTH|         1|\n|   MN|         1|\n+-----+----------+\n~~~\nNext, let’s find the count of businesses by state and sort the result by count in descending order.\n\n**Language-Integrated Query**\n~~~\nval resultDF \u003d biz.groupBy(\"state\").count\nresultDF.orderBy($\"count\".desc).show(5)\n~~~\n**SQL**\n~~~\nsqlContext.sql(\"SELECT state, count(1) as businesses FROM biz GROUP BY state ORDER BY businesses DESC\").show(5)\n\n+-----+-------------+\n|state|businesses   |\n+-----+-------------+\n|   AZ|        25230|\n|   NV|        16485|\n|   NC|         4963|\n|   QC|         3921|\n|   PA|         3041|\n+-----+-------------+\n~~~\nonly showing top 5 rows\n\nYou could have also written the language integrated query version as follows:\n~~~\nval resultDF \u003d biz.groupBy(\"state\").count\nresultDF.orderBy(resultDF(\"count\").desc).show(5)\n~~~\nThe difference between the first and second version is how you refer to the count column in the resultDF DataFrame. You can think of $\"count\" as a shortcut for resultDF(\"count\"). If a method or a function expects an argument of type Column, you can use either syntax.\n\nNext, let’s find five businesses with a five-star rating.\n\n**Language-Integrated Query**\n~~~\nbiz.filter(biz(\"stars\") \u003c\u003d\u003e 5.0)\n   .select(\"name\",\"stars\", \"review_count\", \"city\",  \"state\")\n   .show(5)\n~~~\nThe preceding code first uses the filter method to filter the businesses that have average rating of 5.0. The \u003c\u003d\u003e operator does equality test that is safe for null values. After filtering the businesses, it selects a subset of the columns that are of interest. Finally, it displays five businesses on the console.\n\n**SQL**\n~~~\nsqlContext.sql(\"SELECT name, stars, review_count, city, state FROM biz WHERE stars\u003d5.0\").show(5)\n\n+--------------------+-----+------------+------------+-----+\n|                name|stars|review_count|        city|state|\n+--------------------+-----+------------+------------+-----+\n|    Alteration World|  5.0|           5|    Carnegie|   PA|\n|American Buyers D...|  5.0|           3|   Homestead|   PA|\n|Hunan Wok Chinese...|  5.0|           4|West Mifflin|   PA|\n|      Minerva Bakery|  5.0|           7|  McKeesport|   PA|\n|                Vivo|  5.0|           3|    Bellevue|   PA|\n+--------------------+-----+------------+------------+-----+\nonly showing top 5 rows\n~~~\nFor the next example, let’s find three businesses with five stars in Nevada.\n\n**Language Integrated Query**\n~~~\nbiz.filter($\"stars\" \u003c\u003d\u003e 5.0 \u0026\u0026 $\"state\" \u003c\u003d\u003e \"NV\")\n   .select(\"name\",\"stars\", \"review_count\", \"city\",  \"state\")\n   .show(3)\n~~~\n**SQL**\n~~~\nsqlContext.sql(\"SELECT name, stars, review_count, city, state FROM biz WHERE state \u003d \u0027NV\u0027 AND stars \u003d 5.0\").show(3)\n\n+--------------------+-----+------------+---------+-----+\n|                name|stars|review_count|     city|state|\n+--------------------+-----+------------+---------+-----+\n|              Adiamo|  5.0|           4|Henderson|   NV|\n|CD Young\u0027s Profes...|  5.0|           8|Henderson|   NV|\n|Liaisons Salon \u0026 Spa|  5.0|           5|Henderson|   NV|\n+--------------------+-----+------------+---------+-----+\nonly showing top 3 rows\n~~~\nNext, let’s find the total number of reviews in each state.\n\n**Language Integrated Query**\n~~~\nbiz.groupBy(\"state\").sum(\"review_count\").show()\n~~~\n**SQL**\n~~~\nsqlContext.sql(\"SELECT state, sum(review_count) as reviews FROM biz GROUP BY state\").show()\n\n+-----+-------+\n|state|reviews|\n+-----+-------+\n|  XGL|      3|\n|   NC| 102495|\n|   NV| 752904|\n|   AZ| 636779|\n...\n...\n|   QC|  54569|\n|  KHL|      8|\n|   RP|     75|\n+-----+-------+\nonly showing top 20 rows\n~~~\nNext, let’s find count of businesses by stars.\n\n**Language Integrated Query**\n~~~\nbiz.groupBy(\"stars\").count.show()\n~~~\n**SQL**\n~~~\nsqlContext.sql(\"SELECT stars, count(1) as businesses FROM biz GROUP BY stars\").show()\n\n+-----+----------+\n|stars|businesses|\n+-----+----------+\n|  1.0|       637|\n|  3.5|     13171|\n|  4.5|      9542|\n|  3.0|      8335|\n|  1.5|      1095|\n|  5.0|      7354|\n|  2.5|      5211|\n|  4.0|     13475|\n|  2.0|      2364|\n+-----+----------+\n~~~\nNext, let’s find the average number of reviews for a business by state.\n\n**Language-Integrated Query**\n~~~\nval avgReviewsByState \u003d biz.groupBy(\"state\").avg(\"review_count\")\navgReviewsByState.show()\n~~~\n**SQL**\n~~~\nsqlContext.sql(\"SELECT state, AVG(review_count) as avg_reviews FROM biz GROUP BY state\").show()\n\n+-----+------------------+\n|state|       avg_reviews|\n+-----+------------------+\n|  XGL|               3.0|\n|   NC|20.651823493854522|\n|   NV| 45.67206551410373|\n|   AZ|25.238961553705906|\n...\n...\n|   QC|13.917112981382301|\n|  KHL|               8.0|\n|   RP| 5.769230769230769|\n+-----+------------------+\nonly showing top 20 rows\n~~~\nNext, let’s find the top five states by the average number of reviews for a business.\n\n**Language Integrated Query**\n~~~\nbiz.groupBy(\"state\")\n   .avg(\"review_count\")\n   .withColumnRenamed(\"AVG(review_count)\", \"rc\")\n   .orderBy($\"rc\".desc)\n   .selectExpr(\"state\", \"ROUND(rc) as avg_reviews\")\n   .show(5)\n~~~\n**SQL**\n~~~\nsqlContext.sql(\"SELECT state, ROUND(AVG(review_count)) as avg_reviews FROM biz GROUP BY state ORDER BY avg_reviews DESC LIMIT 5\").show()\n\n+-----+-----------+\n|state|avg_reviews|\n+-----+-----------+\n|   NV|       46.0|\n|   AZ|       25.0|\n|   PA|       24.0|\n|   NC|       21.0|\n|   IL|       21.0|\n+-----+-----------+\n~~~\nNext, let’s find the top 5 businesses in Las Vegas by average stars and review counts.\n\n**Language Integrated Query**\n~~~\nbiz.filter($\"city\" \u003d\u003d\u003d \"Las Vegas\")\n   .sort($\"stars\".desc, $\"review_count\".desc)\n   .select($\"name\", $\"stars\", $\"review_count\")\n   .show(5)\n~~~\n**SQL**\n~~~\nsqlContext.sql(\"SELECT name, stars, review_count FROM biz WHERE city \u003d \u0027Las Vegas\u0027 ORDER BY stars DESC, review_count DESC LIMIT 5 \").show\n\n+--------------------+-----+------------+\n|                name|stars|review_count|\n+--------------------+-----+------------+\n|      Art of Flavors|  5.0|         321|\n|Free Vegas Club P...|  5.0|         285|\n|Fabulous Eyebrow ...|  5.0|         244|\n|Raiding The Rock ...|  5.0|         199|\n|            Eco-Tint|  5.0|         193|\n+--------------------+-----+------------+\n~~~\nNext, let’s write the data in Parquet format.\n~~~\nbiz.write.mode(\"overwrite\").parquet(\"path/to/yelp_business.parquet\")\n~~~\nYou can read the Parquet files created in the previous step, as shown next.\n~~~\nval ybDF \u003d sqlContext.read.parquet(\"path/to/yelp_business.parquet\")\n~~~\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 2:47:42 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eUDFs and UDAFs\u003c/h2\u003e\n\u003cp\u003eSpark SQL allows user-defined functions (UDFs) and user-defined aggregation functions (UDAFs). Both UDFs and UDAFs perform custom computations on a dataset. A \u003cstrong\u003eUDF performs custom computation one row at a time and returns a value for each row\u003c/strong\u003e. A \u003cstrong\u003eUDAF applies custom aggregation over groups of rows\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eUDFs and UDAFs can be used just like the built-in functions after they have been registered with Spark SQL.\u003c/p\u003e\n\u003cp\u003eWriting a UDF is as easy as: \u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  def setupUDFs(sqlCtx: SQLContext) \u003d {\n    sqlCtx.udf.register(\u0026quot;strLen\u0026quot;, (s: String) \u003d\u0026gt; s.length())\n  }\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAn UDAF is a little tricky:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  def setupUDAFs(sqlCtx: SQLContext) \u003d {\n    class Avg extends UserDefinedAggregateFunction {\n      // Input type\n      def inputSchema: org.apache.spark.sql.types.StructType \u003d\n        StructType(StructField(\u0026quot;value\u0026quot;, DoubleType) :: Nil)\n\n      def bufferSchema: StructType \u003d StructType(\n        StructField(\u0026quot;count\u0026quot;, LongType) ::\n        StructField(\u0026quot;sum\u0026quot;, DoubleType) :: Nil\n      )\n\n      // Return type\n      def dataType: DataType \u003d DoubleType\n\n      def deterministic: Boolean \u003d true\n\n      def initialize(buffer: MutableAggregationBuffer): Unit \u003d {\n        buffer(0) \u003d 0L\n        buffer(1) \u003d 0.0\n      }\n\n      def update(buffer: MutableAggregationBuffer,input: Row): Unit \u003d {\n        buffer(0) \u003d buffer.getAs[Long](0) + 1\n        buffer(1) \u003d buffer.getAs[Double](1) + input.getAs[Double](0)\n      }\n\n      def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit \u003d {\n        buffer1(0) \u003d buffer1.getAs[Long](0) + buffer2.getAs[Long](0)\n        buffer1(1) \u003d buffer1.getAs[Double](1) + buffer2.getAs[Double](1)\n      }\n\n      def evaluate(buffer: Row): Any \u003d {\n        buffer.getDouble(1) / buffer.getLong(0)\n      }\n    }\n    // Optionally register\n    val avg \u003d new Avg\n    sqlCtx.udf.register(\u0026quot;ourAvg\u0026quot;, avg)\n  }\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eInteractive Analysis Example\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAs you have seen in previous sections, Spark SQL can be used as an interactive data analysis tool. The previous sections used toy examples so that they were easy to understand. This section shows how Spark SQL can be used for interactive data analysis on a real-world dataset. You can use either language integrated queries or SQL/HiveQL for data analysis. This section demonstrates how they can be used interchangeably.\u003c/p\u003e\n\u003cp\u003eThe dataset that will be used is the Yelp challenge dataset. You can download it from www.yelp.com/dataset_challenge. It includes a number of data files. The example uses the yelp_academic_dataset_business.json file, which contains information about businesses. It contains a JSON object per line. Each JSON object contains information about a business, including its name, city, state, reviews, average rating, category and other attributes. You can learn more details about the dataset on Yelp’s website.\u003c/p\u003e\n\u003cp\u003eLet’s launch the Spark shell from a terminal, if it is not already running.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epath/to/spark/bin/spark-shell --master local[*]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAs discussed previously, the \u0026ndash;master argument specifies the Spark master to which the Spark shell should connect. For the examples in this section, you can use Spark in local mode. If you have a real Spark cluster, change the master URL argument to point to your Spark master. The rest of the code in this section does not require any change.\u003c/p\u003e\n\u003cp\u003eFor readability, I have split some of the example code statements into multiple lines. However, the Spark shell executes a statement as soon as you press the ENTER key. For multiline code statements, you can use Spark shell’s paste mode (:paste). Alternatively, enter a complete statement on a single line.\u003c/p\u003e\n\u003cp\u003eConsider the following example.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebiz.filter(\u0026quot;...\u0026quot;)\n   .select(\u0026quot;...\u0026quot;)\n   .show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn the Spark shell, type it without the line breaks, as shown next.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebiz.filter(\u0026quot;...\u0026quot;).select(\u0026quot;...\u0026quot;).show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eConsider another example.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.sql(\u0026quot;SELECT x, count(y) as total FROM t\n                GROUP BY x\n                ORDER BY total\u0026quot;)\n           .show(50)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eType the preceding code in the Spark shell without line breaks, as shown next.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.sql(\u0026quot;SELECT x, count(y) as total FROM t GROUP BY x ORDER BY total \u0026quot;).show(50)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet’s get started now.\u003c/p\u003e\n\u003cp\u003eSince you will be using a few classes and functions from the Spark SQL library, you need the following import statement.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport org.apache.spark.sql._\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet’s create a DataFrame from the Yelp businesses dataset.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval biz \u003d sqlContext.read.json(\u0026quot;path/to/yelp_academic_dataset_business.json\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe preceding statement is equivalent to this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval biz \u003d sqlContext.read.format(\u0026quot;json\u0026quot;).load(\u0026quot;path/to/yelp_academic_dataset_business.json\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMake sure that you specify the correct file path. Replace “path/to” with the name of the directory where you unpacked the Yelp dataset. Spark SQL will throw an exception if it cannot find the file at the specified path.\u003c/p\u003e\n\u003cp\u003eSpark SQL reads the entire dataset once to infer the schema from a JSON file. Let’s check the schema inferred by Spark SQL.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebiz.printSchema()\n\nPartial output is shown next.\n\nroot\n |-- attributes: struct (nullable \u003d true)\n |    |-- Accepts Credit Cards: string (nullable \u003d true)\n |    |-- Ages Allowed: string (nullable \u003d true)\n |    |-- Alcohol: string (nullable \u003d true)\n ...\n ...\n ...\n |-- name: string (nullable \u003d true)\n |-- open: boolean (nullable \u003d true)\n |-- review_count: long (nullable \u003d true)\n |-- stars: double (nullable \u003d true)\n |-- city: string (nullable \u003d true)\n |-- state: string (nullable \u003d true)\n |-- type: string (nullable \u003d true)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo use the SQL/HiveQL interface, you need to register the biz DataFrame as a temporary table.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebiz.registerTempTable(\u0026quot;biz\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow you can analyze the Yelp businesses dataset using either the DataFrame API or SQL/HiveQL. I will show the code for both, but show the output only for the SQL/HiveQL queries.\u003c/p\u003e\n\u003cp\u003eLet’s cache the data in memory since you will be querying it more than one once.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage-Integrated Query\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebiz.cache()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSQL\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.cacheTable(\u0026quot;biz\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can also cache a table using the CACHE TABLE statement, as shown next.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.sql(\u0026quot;CACHE TABLE biz\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that unlike RDD caching, Spark SQL immediately caches a table when you use the CACHE TABLE statement.\u003c/p\u003e\n\u003cp\u003eSince the goal of this section is to just demonstrate how you can use the Spark SQL library for interactive data analysis, the steps from this point onward have no specific ordering. I chose a few seemingly random queries. You may analyze data in a different sequence. Before each example, I will state the goal and then show the Spark SQL code for achieving that goal.\u003c/p\u003e\n\u003cp\u003eLet’s find the number of businesses in the dataset.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage-Integrated Query\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval count \u003d biz.count()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSQL\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.sql(\u0026quot;SELECT count(1) as businesses FROM biz\u0026quot;).show\n\n+----------+\n|businesses|\n+----------+\n|     61184|\n+----------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, let’s find the count of businesses by state.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage-Integrated Query\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval bizCountByState \u003d biz.groupBy(\u0026quot;state\u0026quot;).count\nbizCountByState.show(50)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSQL\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.sql(\u0026quot;SELECT state, count(1) as businesses FROM biz GROUP BY state\u0026quot;).show(50)\n\n+-----+----------+\n|state|businesses|\n+-----+----------+\n|  XGL|         1|\n|   NC|      4963|\n|   NV|     16485|\n|   AZ|     25230|\n...\n...\n|  MLN|       123|\n|  NTH|         1|\n|   MN|         1|\n+-----+----------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, let’s find the count of businesses by state and sort the result by count in descending order.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage-Integrated Query\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval resultDF \u003d biz.groupBy(\u0026quot;state\u0026quot;).count\nresultDF.orderBy($\u0026quot;count\u0026quot;.desc).show(5)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSQL\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.sql(\u0026quot;SELECT state, count(1) as businesses FROM biz GROUP BY state ORDER BY businesses DESC\u0026quot;).show(5)\n\n+-----+-------------+\n|state|businesses   |\n+-----+-------------+\n|   AZ|        25230|\n|   NV|        16485|\n|   NC|         4963|\n|   QC|         3921|\n|   PA|         3041|\n+-----+-------------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eonly showing top 5 rows\u003c/p\u003e\n\u003cp\u003eYou could have also written the language integrated query version as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval resultDF \u003d biz.groupBy(\u0026quot;state\u0026quot;).count\nresultDF.orderBy(resultDF(\u0026quot;count\u0026quot;).desc).show(5)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe difference between the first and second version is how you refer to the count column in the resultDF DataFrame. You can think of $\u0026ldquo;count\u0026rdquo; as a shortcut for resultDF(\u0026ldquo;count\u0026rdquo;). If a method or a function expects an argument of type Column, you can use either syntax.\u003c/p\u003e\n\u003cp\u003eNext, let’s find five businesses with a five-star rating.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage-Integrated Query\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebiz.filter(biz(\u0026quot;stars\u0026quot;) \u0026lt;\u003d\u0026gt; 5.0)\n   .select(\u0026quot;name\u0026quot;,\u0026quot;stars\u0026quot;, \u0026quot;review_count\u0026quot;, \u0026quot;city\u0026quot;,  \u0026quot;state\u0026quot;)\n   .show(5)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe preceding code first uses the filter method to filter the businesses that have average rating of 5.0. The \u0026lt;\u003d\u0026gt; operator does equality test that is safe for null values. After filtering the businesses, it selects a subset of the columns that are of interest. Finally, it displays five businesses on the console.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSQL\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.sql(\u0026quot;SELECT name, stars, review_count, city, state FROM biz WHERE stars\u003d5.0\u0026quot;).show(5)\n\n+--------------------+-----+------------+------------+-----+\n|                name|stars|review_count|        city|state|\n+--------------------+-----+------------+------------+-----+\n|    Alteration World|  5.0|           5|    Carnegie|   PA|\n|American Buyers D...|  5.0|           3|   Homestead|   PA|\n|Hunan Wok Chinese...|  5.0|           4|West Mifflin|   PA|\n|      Minerva Bakery|  5.0|           7|  McKeesport|   PA|\n|                Vivo|  5.0|           3|    Bellevue|   PA|\n+--------------------+-----+------------+------------+-----+\nonly showing top 5 rows\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor the next example, let’s find three businesses with five stars in Nevada.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage Integrated Query\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebiz.filter($\u0026quot;stars\u0026quot; \u0026lt;\u003d\u0026gt; 5.0 \u0026amp;\u0026amp; $\u0026quot;state\u0026quot; \u0026lt;\u003d\u0026gt; \u0026quot;NV\u0026quot;)\n   .select(\u0026quot;name\u0026quot;,\u0026quot;stars\u0026quot;, \u0026quot;review_count\u0026quot;, \u0026quot;city\u0026quot;,  \u0026quot;state\u0026quot;)\n   .show(3)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSQL\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.sql(\u0026quot;SELECT name, stars, review_count, city, state FROM biz WHERE state \u003d \u0026#39;NV\u0026#39; AND stars \u003d 5.0\u0026quot;).show(3)\n\n+--------------------+-----+------------+---------+-----+\n|                name|stars|review_count|     city|state|\n+--------------------+-----+------------+---------+-----+\n|              Adiamo|  5.0|           4|Henderson|   NV|\n|CD Young\u0026#39;s Profes...|  5.0|           8|Henderson|   NV|\n|Liaisons Salon \u0026amp; Spa|  5.0|           5|Henderson|   NV|\n+--------------------+-----+------------+---------+-----+\nonly showing top 3 rows\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, let’s find the total number of reviews in each state.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage Integrated Query\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebiz.groupBy(\u0026quot;state\u0026quot;).sum(\u0026quot;review_count\u0026quot;).show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSQL\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.sql(\u0026quot;SELECT state, sum(review_count) as reviews FROM biz GROUP BY state\u0026quot;).show()\n\n+-----+-------+\n|state|reviews|\n+-----+-------+\n|  XGL|      3|\n|   NC| 102495|\n|   NV| 752904|\n|   AZ| 636779|\n...\n...\n|   QC|  54569|\n|  KHL|      8|\n|   RP|     75|\n+-----+-------+\nonly showing top 20 rows\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, let’s find count of businesses by stars.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage Integrated Query\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebiz.groupBy(\u0026quot;stars\u0026quot;).count.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSQL\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.sql(\u0026quot;SELECT stars, count(1) as businesses FROM biz GROUP BY stars\u0026quot;).show()\n\n+-----+----------+\n|stars|businesses|\n+-----+----------+\n|  1.0|       637|\n|  3.5|     13171|\n|  4.5|      9542|\n|  3.0|      8335|\n|  1.5|      1095|\n|  5.0|      7354|\n|  2.5|      5211|\n|  4.0|     13475|\n|  2.0|      2364|\n+-----+----------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, let’s find the average number of reviews for a business by state.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage-Integrated Query\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval avgReviewsByState \u003d biz.groupBy(\u0026quot;state\u0026quot;).avg(\u0026quot;review_count\u0026quot;)\navgReviewsByState.show()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSQL\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.sql(\u0026quot;SELECT state, AVG(review_count) as avg_reviews FROM biz GROUP BY state\u0026quot;).show()\n\n+-----+------------------+\n|state|       avg_reviews|\n+-----+------------------+\n|  XGL|               3.0|\n|   NC|20.651823493854522|\n|   NV| 45.67206551410373|\n|   AZ|25.238961553705906|\n...\n...\n|   QC|13.917112981382301|\n|  KHL|               8.0|\n|   RP| 5.769230769230769|\n+-----+------------------+\nonly showing top 20 rows\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, let’s find the top five states by the average number of reviews for a business.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage Integrated Query\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebiz.groupBy(\u0026quot;state\u0026quot;)\n   .avg(\u0026quot;review_count\u0026quot;)\n   .withColumnRenamed(\u0026quot;AVG(review_count)\u0026quot;, \u0026quot;rc\u0026quot;)\n   .orderBy($\u0026quot;rc\u0026quot;.desc)\n   .selectExpr(\u0026quot;state\u0026quot;, \u0026quot;ROUND(rc) as avg_reviews\u0026quot;)\n   .show(5)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSQL\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.sql(\u0026quot;SELECT state, ROUND(AVG(review_count)) as avg_reviews FROM biz GROUP BY state ORDER BY avg_reviews DESC LIMIT 5\u0026quot;).show()\n\n+-----+-----------+\n|state|avg_reviews|\n+-----+-----------+\n|   NV|       46.0|\n|   AZ|       25.0|\n|   PA|       24.0|\n|   NC|       21.0|\n|   IL|       21.0|\n+-----+-----------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, let’s find the top 5 businesses in Las Vegas by average stars and review counts.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage Integrated Query\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebiz.filter($\u0026quot;city\u0026quot; \u003d\u003d\u003d \u0026quot;Las Vegas\u0026quot;)\n   .sort($\u0026quot;stars\u0026quot;.desc, $\u0026quot;review_count\u0026quot;.desc)\n   .select($\u0026quot;name\u0026quot;, $\u0026quot;stars\u0026quot;, $\u0026quot;review_count\u0026quot;)\n   .show(5)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSQL\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esqlContext.sql(\u0026quot;SELECT name, stars, review_count FROM biz WHERE city \u003d \u0026#39;Las Vegas\u0026#39; ORDER BY stars DESC, review_count DESC LIMIT 5 \u0026quot;).show\n\n+--------------------+-----+------------+\n|                name|stars|review_count|\n+--------------------+-----+------------+\n|      Art of Flavors|  5.0|         321|\n|Free Vegas Club P...|  5.0|         285|\n|Fabulous Eyebrow ...|  5.0|         244|\n|Raiding The Rock ...|  5.0|         199|\n|            Eco-Tint|  5.0|         193|\n+--------------------+-----+------------+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext, let’s write the data in Parquet format.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebiz.write.mode(\u0026quot;overwrite\u0026quot;).parquet(\u0026quot;path/to/yelp_business.parquet\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can read the Parquet files created in the previous step, as shown next.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval ybDF \u003d sqlContext.read.parquet(\u0026quot;path/to/yelp_business.parquet\u0026quot;)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488330905492_1939893892",
      "id": "20170301-021505_259381059",
      "dateCreated": "Mar 1, 2017 2:15:05 AM",
      "dateStarted": "Mar 1, 2017 2:47:42 AM",
      "dateFinished": "Mar 1, 2017 2:47:43 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nInteractive Analysis with Spark SQL JDBC Server\n---\n\nThis section shows how you can explore the Yelp dataset using just SQL/HiveQL. Scala is not used at all. For this analysis, you will use the Spark SQL Thrift/JDBC/ODBC server and the Beeline client. Both come prepackaged with Spark.\n\nThe first step is to launch the Spark SQL Thrift/JDBC/ODBC server from a terminal. Spark’s sbin directory contains a script for launching it.\n~~~\npath/to/spark/sbin/start-thriftserver.sh --master local[*]\n~~~\nThe second step is to launch Beeline, which is a CLI (command line interface) client for Spark SQL Thrift/JDBC server. Conceptually, it is similar to the mysql client for MySQL or psql client for PostgreSQL. It allows you to type a HiveQL query, sends the typed query to a Spark SQL Thrift/JDBC server for execution, and displays the results on the console.\n\nSpark’s bin directory contains a script for launching Beeline. Let’s open another terminal and launch Beeline.\n~~~\npath/to/spark/bin/beeline\n~~~\nYou should be now inside the Beeline shell and see the Beeline prompt, as shown next.\n~~~\nBeeline version 1.5.2 by Apache Hive\nbeeline\u003e\n~~~\nThe third step is to connect to the Spark SQL Thrift/JDBC server from the Beeline shell.\n~~~\nbeeline\u003e!connect jdbc:hive2://localhost:10000\n~~~\nThe connect command requires a JDBC URL. The default JDBC port for the Spark SQL Thrift/JDBC server is 10000.\n\nBeeline will ask for a username and password. Enter the username that you use to login into your system and a blank password.\n~~~\nbeeline\u003e !connect jdbc:hive2://localhost:10000\n\nscan complete in 26ms\nConnecting to jdbc:hive2://localhost:10000\nEnter username for jdbc:hive2://localhost:10000: your-user-name\nEnter password for jdbc:hive2://localhost:10000:\nConnected to: Spark SQL (version 1.5.2)\nDriver: Spark Project Core (version 1.5.2)\nTransaction isolation: TRANSACTION_REPEATABLE_READ\n0: jdbc:hive2://localhost:10000\u003e\n~~~\nAt this point, you have an active connection between the Beeline client and Spark SQL server.\n\nHowever, the Spark SQL server does not yet know about the Yelp dataset. Let’s create a temporary table that points to the Yelp dataset.\n~~~\n0: jdbc:hive2://localhost:10000\u003e CREATE TEMPORARY TABLE biz USING org.apache.spark.sql.json OPTIONS (path \"path/to/yelp_academic_dataset_business.json\");\n~~~\nMake sure to replace “path/to” with the path for the directory where you unpacked the Yelp dataset on your machine. Spark SQL will throw an exception if it cannot find the file at the specified path.\n\nThe CREATE TEMPORARY TABLE command creates an external temporary table in Hive metastore. A temporary table exists only while the Spark SQL JDBC server is running.\n\nNow you can analyze the Yelp dataset using just plain SQL/HiveQL queries. A few examples are shown next.\n~~~\n0: jdbc:hive2://localhost:10000\u003e SHOW TABLES;\n\n+------------+--------------+--+\n| tableName  | isTemporary  |\n+------------+--------------+--+\n| biz        | true         |\n+------------+--------------+--+\n~~~\n\n~~~\n0: jdbc:hive2://localhost:10000\u003e SELECT count(1) from biz;\n\n+--------+--+\n|  _c0   |\n+--------+--+\n| 61184  |\n+--------+--+\n~~~\n~~~\n0: jdbc:hive2://localhost:10000\u003e SELECT state, count(1) as cnt FROM biz GROUP BY state ORDER BY cnt DESC LIMIT 5;\n\n+--------+--------+--+\n| state  |  cnt   |\n+--------+--------+--+\n| AZ     | 25230  |\n| NV     | 16485  |\n| NC     | 4963   |\n| QC     | 3921   |\n| PA     | 3041   |\n+--------+--------+--+\n5 rows selected (3.241 seconds)\n~~~\n~~~\n0: jdbc:hive2://localhost:10000\u003e SELECT state, count(1) as businesses, sum(review_count) as reviews FROM biz GROUP BY state ORDER BY businesses DESC LIMIT 5;\n\n+--------+-------------+----------+--+\n| state  | businesses  | reviews  |\n+--------+-------------+----------+--+\n| AZ     | 25230       | 636779   |\n| NV     | 16485       | 752904   |\n| NC     | 4963        | 102495   |\n| QC     | 3921        | 54569    |\n| PA     | 3041        | 72409    |\n+--------+-------------+----------+--+\n5 rows selected (1.293 seconds)\n~~~\n~~~\n0: jdbc:hive2://localhost:10000\u003e SELECT name, review_count, stars, city, state from biz WHERE stars \u003d 5.0 ORDER BY review_count DESC LIMIT 5;\n\n+----------------------------+---------------+--------+-------------+--------+--+\n|            name            | review_count  | stars  |    city     | state  |\n+----------------------------+---------------+--------+-------------+--------+--+\n| Art of Flavors             | 321           | 5.0    | Las Vegas   | NV     |\n| PNC Park                   | 306           | 5.0    | Pittsburgh  | PA     |\n| Gaucho Parrilla Argentina  | 286           | 5.0    | Pittsburgh  | PA     |\n| Free Vegas Club Passes     | 285           | 5.0    | Las Vegas   | NV     |\n| Little Miss BBQ            | 267           | 5.0    | Phoenix     | AZ     |\n+----------------------------+---------------+--------+-------------+--------+--+\n5 rows selected (0.511 seconds)\n~~~\n\n\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 2:55:37 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {},
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eInteractive Analysis with Spark SQL JDBC Server\u003c/h2\u003e\n\u003cp\u003eThis section shows how you can explore the Yelp dataset using just SQL/HiveQL. Scala is not used at all. For this analysis, you will use the Spark SQL Thrift/JDBC/ODBC server and the Beeline client. Both come prepackaged with Spark.\u003c/p\u003e\n\u003cp\u003eThe first step is to launch the Spark SQL Thrift/JDBC/ODBC server from a terminal. Spark’s sbin directory contains a script for launching it.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epath/to/spark/sbin/start-thriftserver.sh --master local[*]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe second step is to launch Beeline, which is a CLI (command line interface) client for Spark SQL Thrift/JDBC server. Conceptually, it is similar to the mysql client for MySQL or psql client for PostgreSQL. It allows you to type a HiveQL query, sends the typed query to a Spark SQL Thrift/JDBC server for execution, and displays the results on the console.\u003c/p\u003e\n\u003cp\u003eSpark’s bin directory contains a script for launching Beeline. Let’s open another terminal and launch Beeline.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epath/to/spark/bin/beeline\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou should be now inside the Beeline shell and see the Beeline prompt, as shown next.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBeeline version 1.5.2 by Apache Hive\nbeeline\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe third step is to connect to the Spark SQL Thrift/JDBC server from the Beeline shell.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebeeline\u0026gt;!connect jdbc:hive2://localhost:10000\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe connect command requires a JDBC URL. The default JDBC port for the Spark SQL Thrift/JDBC server is 10000.\u003c/p\u003e\n\u003cp\u003eBeeline will ask for a username and password. Enter the username that you use to login into your system and a blank password.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebeeline\u0026gt; !connect jdbc:hive2://localhost:10000\n\nscan complete in 26ms\nConnecting to jdbc:hive2://localhost:10000\nEnter username for jdbc:hive2://localhost:10000: your-user-name\nEnter password for jdbc:hive2://localhost:10000:\nConnected to: Spark SQL (version 1.5.2)\nDriver: Spark Project Core (version 1.5.2)\nTransaction isolation: TRANSACTION_REPEATABLE_READ\n0: jdbc:hive2://localhost:10000\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAt this point, you have an active connection between the Beeline client and Spark SQL server.\u003c/p\u003e\n\u003cp\u003eHowever, the Spark SQL server does not yet know about the Yelp dataset. Let’s create a temporary table that points to the Yelp dataset.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e0: jdbc:hive2://localhost:10000\u0026gt; CREATE TEMPORARY TABLE biz USING org.apache.spark.sql.json OPTIONS (path \u0026quot;path/to/yelp_academic_dataset_business.json\u0026quot;);\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eMake sure to replace “path/to” with the path for the directory where you unpacked the Yelp dataset on your machine. Spark SQL will throw an exception if it cannot find the file at the specified path.\u003c/p\u003e\n\u003cp\u003eThe CREATE TEMPORARY TABLE command creates an external temporary table in Hive metastore. A temporary table exists only while the Spark SQL JDBC server is running.\u003c/p\u003e\n\u003cp\u003eNow you can analyze the Yelp dataset using just plain SQL/HiveQL queries. A few examples are shown next.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e0: jdbc:hive2://localhost:10000\u0026gt; SHOW TABLES;\n\n+------------+--------------+--+\n| tableName  | isTemporary  |\n+------------+--------------+--+\n| biz        | true         |\n+------------+--------------+--+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e0: jdbc:hive2://localhost:10000\u0026gt; SELECT count(1) from biz;\n\n+--------+--+\n|  _c0   |\n+--------+--+\n| 61184  |\n+--------+--+\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e0: jdbc:hive2://localhost:10000\u0026gt; SELECT state, count(1) as cnt FROM biz GROUP BY state ORDER BY cnt DESC LIMIT 5;\n\n+--------+--------+--+\n| state  |  cnt   |\n+--------+--------+--+\n| AZ     | 25230  |\n| NV     | 16485  |\n| NC     | 4963   |\n| QC     | 3921   |\n| PA     | 3041   |\n+--------+--------+--+\n5 rows selected (3.241 seconds)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e0: jdbc:hive2://localhost:10000\u0026gt; SELECT state, count(1) as businesses, sum(review_count) as reviews FROM biz GROUP BY state ORDER BY businesses DESC LIMIT 5;\n\n+--------+-------------+----------+--+\n| state  | businesses  | reviews  |\n+--------+-------------+----------+--+\n| AZ     | 25230       | 636779   |\n| NV     | 16485       | 752904   |\n| NC     | 4963        | 102495   |\n| QC     | 3921        | 54569    |\n| PA     | 3041        | 72409    |\n+--------+-------------+----------+--+\n5 rows selected (1.293 seconds)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e0: jdbc:hive2://localhost:10000\u0026gt; SELECT name, review_count, stars, city, state from biz WHERE stars \u003d 5.0 ORDER BY review_count DESC LIMIT 5;\n\n+----------------------------+---------------+--------+-------------+--------+--+\n|            name            | review_count  | stars  |    city     | state  |\n+----------------------------+---------------+--------+-------------+--------+--+\n| Art of Flavors             | 321           | 5.0    | Las Vegas   | NV     |\n| PNC Park                   | 306           | 5.0    | Pittsburgh  | PA     |\n| Gaucho Parrilla Argentina  | 286           | 5.0    | Pittsburgh  | PA     |\n| Free Vegas Club Passes     | 285           | 5.0    | Las Vegas   | NV     |\n| Little Miss BBQ            | 267           | 5.0    | Phoenix     | AZ     |\n+----------------------------+---------------+--------+-------------+--------+--+\n5 rows selected (0.511 seconds)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488332092933_-2042204150",
      "id": "20170301-023452_1009930393",
      "dateCreated": "Mar 1, 2017 2:34:52 AM",
      "dateStarted": "Mar 1, 2017 2:55:15 AM",
      "dateFinished": "Mar 1, 2017 2:55:15 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nHere is the APIs comparison between RDD, Dataframe and Dataset.\n---\n\n**RDD**\n\nThe main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel.\n\n**RDD Features:**\n\n* Distributed collection:\nRDD uses MapReduce operations which is widely adopted for processing and generating large datasets with a parallel, distributed algorithm on a cluster. It allows users to write parallel computations, using a set of high-level operators, without having to worry about work distribution and fault tolerance.\n\n* Immutable: RDDs composed of a collection of records which are partitioned. A partition is a basic unit of parallelism in an RDD, and each partition is one logical division of data which is immutable and created through some transformations on existing partitions.Immutability helps to achieve consistency in computations.\n\n* Fault tolerant: In a case of we lose some partition of RDD , we can replay the transformation on that partition in lineage to achieve the same computation, rather than doing data replication across multiple nodes.This characteristic is the biggest benefit of RDD because it saves a lot of efforts in data management and replication and thus achieves faster computations.\n\n* Lazy evaluations: All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset . The transformations are only computed when an action requires a result to be returned to the driver program.\n\n* Functional transformations: RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset.\n\n* Data processing formats:\nIt can easily and efficiently process data which is structured as well as unstructured data.\n\n*Programming Languages supported:\nRDD API is available in Java, Scala, Python and R.\n\n**RDD Limitations:**\n\n* No inbuilt optimization engine: When working with structured data, RDDs cannot take advantages of Spark’s advanced optimizers including catalyst optimizer and Tungsten execution engine. Developers need to optimize each RDD based on its attributes.\n\n* Handling structured data: Unlike Dataframe and datasets, RDDs don’t infer the schema of the ingested data and requires the user to specify it.\n\n**Dataframes**\n\nSpark introduced Dataframes in Spark 1.3 release. Dataframe overcomes the key challenges that RDDs had.\n\nA DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a R/Python Dataframe. Along with Dataframe, Spark also introduced catalyst optimizer, which leverages advanced programming features to build an extensible query optimizer.\n\n** Dataframe Features:**\n\n* Distributed collection of Row Object: A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database, but with richer optimizations under the hood.\n\n* Data Processing: Processing structured and unstructured data formats (Avro, CSV, elastic search, and Cassandra) and storage systems (HDFS, HIVE tables, MySQL, etc). It can read and write from all these various datasources.\n\n* Optimization using catalyst optimizer: It powers both SQL queries and the DataFrame API. Dataframe use catalyst tree transformation framework in four phases,\n\n    1.Analyzing a logical plan to resolve references\n    2.Logical plan optimization\n    3.Physical planning\n    4.Code generation to compile parts of the query to Java bytecode.\n\n* Hive Compatibility: Using Spark SQL, you can run unmodified Hive queries on your existing Hive warehouses. It reuses Hive frontend and MetaStore and gives you full compatibility with existing Hive data, queries, and UDFs.\n\n* Tungsten: Tungsten provides a physical execution backend whichexplicitly manages memory and dynamically generates bytecode for expression evaluation.\n\n* Programming Languages supported:\nDataframe API is available in Java, Scala, Python, and R.\n\n**Dataframe Limitations:**\n\nCompile-time type safety: As discussed, Dataframe API does not support compile time safety which limits you from manipulating data when the structure is not know. The following example works during compile time. However, you will get a Runtime exception when executing this code.\n\nExample:\n~~~\ncase class Person(name : String , age : Int) \nval dataframe \u003d sqlContect.read.json(\"people.json\") \ndataframe.filter(\"salary \u003e 10000\").show \n\u003d\u003e throws Exception : cannot resolve \u0027salary\u0027 given input age , name\n~~~\nThis is challenging specially when you are working with several transformation and aggregation steps.\n\nCannot operate on domain Object (lost domain object): Once you have transformed a domain object into dataframe, you cannot regenerate it from it. In the following example, once we have create personDF from personRDD, we won’t be recover the original RDD of Person class (RDD[Person]).\n\nExample:\n~~~\ncase class Person(name : String , age : Int)\nval personRDD \u003d sc.makeRDD(Seq(Person(\"A\",10),Person(\"B\",20)))\nval personDF \u003d sqlContect.createDataframe(personRDD)\npersonDF.rdd // returns RDD[Row] , does not returns RDD[Person]\n~~~\n**Datasets API**\n\nDataset API is an extension to DataFrames that provides a type-safe, object-oriented programming interface. It is a strongly-typed, immutable collection of objects that are mapped to a relational schema.\n\nAt the core of the Dataset, API is a new concept called an encoder, which is responsible for converting between JVM objects and tabular representation. The tabular representation is stored using Spark internal Tungsten binary format, allowing for operations on serialized data and improved memory utilization. Spark 1.6 comes with support for automatically generating encoders for a wide variety of types, including primitive types (e.g. String, Integer, Long), Scala case classes, and Java Beans.\n\n**Dataset Features:**\n\n* Provides best of both RDD and Dataframe: RDD(functional programming, type safe), DataFrame (relational model, Query optimazation , Tungsten execution, sorting and shuffling)\n\n* Encoders: With the use of Encoders, it is easy to convert any JVM object into a Dataset, allowing users to work with both structured and unstructured data unlike Dataframe.\n\n* Programming Languages supported: Datasets API is currently only available in Scala and Java. Python and R are currently not supported in version 1.6. Python support is slated for version 2.0.\n\n* Type Safety: Datasets API provides compile time safety which was not available in Dataframes. In the example below, we can see how Dataset can operate on domain objects with compile lambda functions.\n\nExample:\n~~~\ncase class Person(name : String , age : Int)\nval personRDD \u003d sc.makeRDD(Seq(Person(\"A\",10),Person(\"B\",20)))\nval personDF \u003d sqlContect.createDataframe(personRDD)\nval ds:Dataset[Person] \u003d personDF.as[Person]\nds.filter(p \u003d\u003e p.age \u003e 25)\nds.filter(p \u003d\u003e p.salary \u003e 25)\n // error : value salary is not a member of person\nds.rdd // returns RDD[Person]\n~~~\n* Interoperable: Datasets allows you to easily convert your existing RDDs and Dataframes into datasets without boilerplate code.\n\n**Datasets API Limitation:**\n\nRequires type casting to String: Querying the data from datasets currently requires us to specify the fields in the class as a string. Once we have queried the data, we are forced to cast column to the required data type. On the other hand, if we use map operation on Datasets, it will not use Catalyst optimizer.\n\nExample:\n~~~\nds.select(col(\"name\").as[String], $\"age\".as[Int]).collect()\n~~~\nNo support for Python and R: As of release 1.6, Datasets only support Scala and Java. Python support will be introduced in Spark 2.0.\n\nThe Datasets API brings in several advantages over the existing RDD and Dataframe API with better type safety and functional programming.With the challenge of type casting requirements in the API, you would still not the required type safety and will make your code brittle.\nshareimprove this answer\n\t\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 3:17:12 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eHere is the APIs comparison between RDD, Dataframe and Dataset.\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eRDD\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRDD Features:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003eDistributed collection:\u003cbr/\u003eRDD uses MapReduce operations which is widely adopted for processing and generating large datasets with a parallel, distributed algorithm on a cluster. It allows users to write parallel computations, using a set of high-level operators, without having to worry about work distribution and fault tolerance.\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eImmutable: RDDs composed of a collection of records which are partitioned. A partition is a basic unit of parallelism in an RDD, and each partition is one logical division of data which is immutable and created through some transformations on existing partitions.Immutability helps to achieve consistency in computations.\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eFault tolerant: In a case of we lose some partition of RDD , we can replay the transformation on that partition in lineage to achieve the same computation, rather than doing data replication across multiple nodes.This characteristic is the biggest benefit of RDD because it saves a lot of efforts in data management and replication and thus achieves faster computations.\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eLazy evaluations: All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset . The transformations are only computed when an action requires a result to be returned to the driver program.\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eFunctional transformations: RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset.\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eData processing formats:\u003cbr/\u003eIt can easily and efficiently process data which is structured as well as unstructured data.\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e*Programming Languages supported:\u003cbr/\u003eRDD API is available in Java, Scala, Python and R.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRDD Limitations:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003eNo inbuilt optimization engine: When working with structured data, RDDs cannot take advantages of Spark’s advanced optimizers including catalyst optimizer and Tungsten execution engine. Developers need to optimize each RDD based on its attributes.\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eHandling structured data: Unlike Dataframe and datasets, RDDs don’t infer the schema of the ingested data and requires the user to specify it.\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eDataframes\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSpark introduced Dataframes in Spark 1.3 release. Dataframe overcomes the key challenges that RDDs had.\u003c/p\u003e\n\u003cp\u003eA DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a R/Python Dataframe. Along with Dataframe, Spark also introduced catalyst optimizer, which leverages advanced programming features to build an extensible query optimizer.\u003c/p\u003e\n\u003cp\u003e** Dataframe Features:**\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003eDistributed collection of Row Object: A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database, but with richer optimizations under the hood.\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eData Processing: Processing structured and unstructured data formats (Avro, CSV, elastic search, and Cassandra) and storage systems (HDFS, HIVE tables, MySQL, etc). It can read and write from all these various datasources.\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eOptimization using catalyst optimizer: It powers both SQL queries and the DataFrame API. Dataframe use catalyst tree transformation framework in four phases,\u003c/p\u003e\n    \u003cp\u003e1.Analyzing a logical plan to resolve references\u003cbr/\u003e2.Logical plan optimization\u003cbr/\u003e3.Physical planning\u003cbr/\u003e4.Code generation to compile parts of the query to Java bytecode.\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eHive Compatibility: Using Spark SQL, you can run unmodified Hive queries on your existing Hive warehouses. It reuses Hive frontend and MetaStore and gives you full compatibility with existing Hive data, queries, and UDFs.\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eTungsten: Tungsten provides a physical execution backend whichexplicitly manages memory and dynamically generates bytecode for expression evaluation.\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eProgramming Languages supported:\u003cbr/\u003eDataframe API is available in Java, Scala, Python, and R.\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eDataframe Limitations:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eCompile-time type safety: As discussed, Dataframe API does not support compile time safety which limits you from manipulating data when the structure is not know. The following example works during compile time. However, you will get a Runtime exception when executing this code.\u003c/p\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecase class Person(name : String , age : Int) \nval dataframe \u003d sqlContect.read.json(\u0026quot;people.json\u0026quot;) \ndataframe.filter(\u0026quot;salary \u0026gt; 10000\u0026quot;).show \n\u003d\u0026gt; throws Exception : cannot resolve \u0026#39;salary\u0026#39; given input age , name\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is challenging specially when you are working with several transformation and aggregation steps.\u003c/p\u003e\n\u003cp\u003eCannot operate on domain Object (lost domain object): Once you have transformed a domain object into dataframe, you cannot regenerate it from it. In the following example, once we have create personDF from personRDD, we won’t be recover the original RDD of Person class (RDD[Person]).\u003c/p\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecase class Person(name : String , age : Int)\nval personRDD \u003d sc.makeRDD(Seq(Person(\u0026quot;A\u0026quot;,10),Person(\u0026quot;B\u0026quot;,20)))\nval personDF \u003d sqlContect.createDataframe(personRDD)\npersonDF.rdd // returns RDD[Row] , does not returns RDD[Person]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eDatasets API\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDataset API is an extension to DataFrames that provides a type-safe, object-oriented programming interface. It is a strongly-typed, immutable collection of objects that are mapped to a relational schema.\u003c/p\u003e\n\u003cp\u003eAt the core of the Dataset, API is a new concept called an encoder, which is responsible for converting between JVM objects and tabular representation. The tabular representation is stored using Spark internal Tungsten binary format, allowing for operations on serialized data and improved memory utilization. Spark 1.6 comes with support for automatically generating encoders for a wide variety of types, including primitive types (e.g. String, Integer, Long), Scala case classes, and Java Beans.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDataset Features:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n  \u003cp\u003eProvides best of both RDD and Dataframe: RDD(functional programming, type safe), DataFrame (relational model, Query optimazation , Tungsten execution, sorting and shuffling)\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eEncoders: With the use of Encoders, it is easy to convert any JVM object into a Dataset, allowing users to work with both structured and unstructured data unlike Dataframe.\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eProgramming Languages supported: Datasets API is currently only available in Scala and Java. Python and R are currently not supported in version 1.6. Python support is slated for version 2.0.\u003c/p\u003e\u003c/li\u003e\n  \u003cli\u003e\n  \u003cp\u003eType Safety: Datasets API provides compile time safety which was not available in Dataframes. In the example below, we can see how Dataset can operate on domain objects with compile lambda functions.\u003c/p\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecase class Person(name : String , age : Int)\nval personRDD \u003d sc.makeRDD(Seq(Person(\u0026quot;A\u0026quot;,10),Person(\u0026quot;B\u0026quot;,20)))\nval personDF \u003d sqlContect.createDataframe(personRDD)\nval ds:Dataset[Person] \u003d personDF.as[Person]\nds.filter(p \u003d\u0026gt; p.age \u0026gt; 25)\nds.filter(p \u003d\u0026gt; p.salary \u0026gt; 25)\n // error : value salary is not a member of person\nds.rdd // returns RDD[Person]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n  \u003cli\u003eInteroperable: Datasets allows you to easily convert your existing RDDs and Dataframes into datasets without boilerplate code.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eDatasets API Limitation:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRequires type casting to String: Querying the data from datasets currently requires us to specify the fields in the class as a string. Once we have queried the data, we are forced to cast column to the required data type. On the other hand, if we use map operation on Datasets, it will not use Catalyst optimizer.\u003c/p\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eds.select(col(\u0026quot;name\u0026quot;).as[String], $\u0026quot;age\u0026quot;.as[Int]).collect()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNo support for Python and R: As of release 1.6, Datasets only support Scala and Java. Python support will be introduced in Spark 2.0.\u003c/p\u003e\n\u003cp\u003eThe Datasets API brings in several advantages over the existing RDD and Dataframe API with better type safety and functional programming.With the challenge of type casting requirements in the API, you would still not the required type safety and will make your code brittle.\u003cbr/\u003eshareimprove this answer\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488333823381_1271549471",
      "id": "20170301-030343_86271248",
      "dateCreated": "Mar 1, 2017 3:03:43 AM",
      "dateStarted": "Mar 1, 2017 3:17:12 AM",
      "dateFinished": "Mar 1, 2017 3:17:12 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nSummary\n---\n\nSpark SQL is a Spark library that makes it simple to do fast analysis of structured data. It provides more than just a SQL interface to Spark. It improves Spark usability, developer productivity, and application performance.\n\nSpark SQL provides a unified interface for processing structured data from a variety of data sources. It can be used to process data stored on a local file system, HDFS, S3, JDBC-compliant databases, and NoSQL datastores. It allows you to process data from any of these data sources using SQL, HiveQL, or the DataFrame API.\n\nSpark SQL also provides a distributed SQL query server that supports Thrift, JDBC and ODBC clients. It allows you to interactively analyze data using just SQL/HiveQL. The Spark SQL Thrift/JDBC/ODBC server can be used with any third-party BI or data visualization application that supports the JDBC or ODBC interface.",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 2:56:41 AM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eSummary\u003c/h2\u003e\n\u003cp\u003eSpark SQL is a Spark library that makes it simple to do fast analysis of structured data. It provides more than just a SQL interface to Spark. It improves Spark usability, developer productivity, and application performance.\u003c/p\u003e\n\u003cp\u003eSpark SQL provides a unified interface for processing structured data from a variety of data sources. It can be used to process data stored on a local file system, HDFS, S3, JDBC-compliant databases, and NoSQL datastores. It allows you to process data from any of these data sources using SQL, HiveQL, or the DataFrame API.\u003c/p\u003e\n\u003cp\u003eSpark SQL also provides a distributed SQL query server that supports Thrift, JDBC and ODBC clients. It allows you to interactively analyze data using just SQL/HiveQL. The Spark SQL Thrift/JDBC/ODBC server can be used with any third-party BI or data visualization application that supports the JDBC or ODBC interface.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1488333122823_-815926206",
      "id": "20170301-025202_1923471176",
      "dateCreated": "Mar 1, 2017 2:52:02 AM",
      "dateStarted": "Mar 1, 2017 2:56:35 AM",
      "dateFinished": "Mar 1, 2017 2:56:35 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "Mar 1, 2017 2:56:34 AM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1488333394303_-598944795",
      "id": "20170301-025634_1797782582",
      "dateCreated": "Mar 1, 2017 2:56:34 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "/CursoAmadeus2017/SparkSQL",
  "id": "2CCTD7RNN",
  "angularObjects": {
    "2CC52E4S5:shared_process": [],
    "2C9N1T5Y3:shared_process": [],
    "2CAZ8JZD9:shared_process": [],
    "2CCZDGC3H:shared_process": [],
    "2C9S66NQK:shared_process": [],
    "2C94SRCMC:shared_process": [],
    "2CCCN7GZD:shared_process": [],
    "2CCGRTB7B:shared_process": [],
    "2CC8YMXCQ:shared_process": [],
    "2CC6XAQM3:shared_process": [],
    "2C9731R8U:shared_process": [],
    "2CBZVGS5S:shared_process": [],
    "2CBDXBFEU:shared_process": [],
    "2CB3WM78Y:shared_process": [],
    "2CCN2DPHQ:shared_process": [],
    "2C9GE2AUU:shared_process": [],
    "2CCV4F9FS:shared_process": [],
    "2CCHNGUC9:shared_process": [],
    "2C9RQJYPD:shared_process": []
  },
  "config": {
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}